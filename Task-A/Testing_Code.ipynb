{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12180300,"sourceType":"datasetVersion","datasetId":7671325},{"sourceId":12196727,"sourceType":"datasetVersion","datasetId":7682854},{"sourceId":12198873,"sourceType":"datasetVersion","datasetId":7683237},{"sourceId":12224203,"sourceType":"datasetVersion","datasetId":7701565},{"sourceId":12227476,"sourceType":"datasetVersion","datasetId":7703701},{"sourceId":12363077,"sourceType":"datasetVersion","datasetId":7794333}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clear Working Directory","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the working directory\nworking_dir = '/kaggle/working/'\n\n# Get a list of all items in the working directory\nitems_in_working = os.listdir(working_dir)\n\n# Iterate through the items and remove directories\nprint(f\"Cleaning up {working_dir}...\")\nfor item in items_in_working:\n    item_path = os.path.join(working_dir, item)\n    if os.path.isdir(item_path):\n        try:\n            shutil.rmtree(item_path)\n            print(f\"Removed directory: {item_path}\")\n        except OSError as e:\n            print(f\"Error removing directory {item_path}: {e}\")\n    # Optional: If you also want to remove files, uncomment the else if below\n    # elif os.path.isfile(item_path):\n    #     try:\n    #         os.remove(item_path)\n    #         print(f\"Removed file: {item_path}\")\n    #     except OSError as e:\n    #         print(f\"Error removing file {item_path}: {e}\")\n\nprint(\"Cleanup complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Install necessary libraries","metadata":{}},{"cell_type":"code","source":"!apt-get update\n!apt-get install -y wget","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install opencv-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Give path here","metadata":{}},{"cell_type":"code","source":"CHECKPOINT_PATH = \"/kaggle/input/ckpt-for-comsystaska/best_model_taskA.ckpt\" # Corrected checkpoint name and path\nTEST_INPUT_PATH = '/kaggle/input/comsys-taska/Task_A/val' # Your provided test dataset path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final evaluation","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\nfrom collections import Counter\nfrom sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, auc, classification_report, confusion_matrix, accuracy_score\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport timm\n\n# Suppress specific warnings if needed, e.g., about PIL image conversion or small batches\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='torchvision.transforms.functional_tensor')\n\n# Define paths (Adjust KAGGLE_INPUT_PATH if your base dataset structure is different for train/val)\nKAGGLE_INPUT_PATH = '/kaggle/input/comsys-taska/Task_A'\nPREPROCESSED_PATH = '/kaggle/working/preprocessed_faces' # This is where the preprocessed train/val data was saved\nMODEL_DIR = \"/kaggle/working/face-detection-model-files/\" # Directory for face detection models\n\n\n# Test specific paths (User explicitly defines the full preprocessed output directory for test)\n\nTEST_PREPROCESSED_OUTPUT_DIR = '/kaggle/working/preprocessed_test_faces_output' # <--- MODIFIED HERE: Full, explicit path for test preprocessed output\n\n# --- 0. Setup Face Detection Model Files (re-included for completeness, ensures files exist) ---\ndef setup_face_detection_models():\n    \"\"\"Ensures face detection model files are present.\"\"\"\n    if not os.path.exists(MODEL_DIR):\n        os.makedirs(MODEL_DIR)\n        print(f\"Created directory: {MODEL_DIR}\")\n\n    prototxt_path = os.path.join(MODEL_DIR, \"deploy.prototxt\")\n    caffemodel_path = os.path.join(MODEL_DIR, \"res10_300x300_ssd_iter_140000.caffemodel\")\n\n    if not os.path.exists(prototxt_path):\n        print(f\"Downloading deploy.prototxt to {MODEL_DIR}...\")\n        !wget -P {MODEL_DIR} https://raw.githubusercontent.com/opencv/opencv/4.x/samples/dnn/face_detector/deploy.prototxt\n    else:\n        print(\"deploy.prototxt already exists.\")\n\n    if not os.path.exists(caffemodel_path):\n        print(f\"Downloading res10_300x300_ssd_iter_140000.caffemodel to {MODEL_DIR}...\")\n        !wget -P {MODEL_DIR} https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\n    else:\n        print(\"res10_300x300_ssd_iter_140000.caffemodel already exists.\")\n\n# --- Preprocessing Function ---\ndef preprocess_and_detect_face(input_folder, output_folder):\n    \"\"\"\n    Preprocesses images, detects a single face, and saves the cropped face\n    to a new folder. If no high-confidence face is found, it selects the\n    detection with the highest overall confidence.\n    Args:\n        input_folder (str): Path to the folder containing input images.\n        output_folder (str): Path to the folder where preprocessed and\n                              cropped face images will be saved.\n    \"\"\"\n    try:\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n            print(f\"Created output folder: {output_folder}\")\n        else:\n            print(f\"Output folder already exists: {output_folder}\")\n    except OSError as e:\n        print(f\"Error creating output folder '{output_folder}': {e}\")\n        print(\"Please check your permissions or the specified path.\")\n        return\n\n    prototxt_path = os.path.join(MODEL_DIR, \"deploy.prototxt\")\n    caffemodel_path = os.path.join(MODEL_DIR, \"res10_300x300_ssd_iter_140000.caffemodel\")\n\n    if not os.path.exists(prototxt_path) or not os.path.exists(caffemodel_path):\n        print(\"Error: Pre-trained Caffe model files not found. Please run setup_face_detection_models().\")\n        return\n\n    net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n    print(\"Pre-trained face detection model loaded successfully.\")\n\n    if not os.path.exists(input_folder):\n        print(f\"Error: Input folder '{input_folder}' does not exist. Please check the path.\")\n        return\n\n    # Iterate through gender subfolders (assuming 'male' and 'female')\n    for gender in ['female', 'male']:\n        current_gender_input_folder = os.path.join(input_folder, gender)\n        current_gender_output_folder = os.path.join(output_folder, gender)\n\n        if not os.path.exists(current_gender_input_folder):\n            print(f\"Warning: Gender input folder '{current_gender_input_folder}' not found. Skipping.\")\n            continue\n        \n        # Create output gender folder\n        if not os.path.exists(current_gender_output_folder):\n            os.makedirs(current_gender_output_folder)\n            print(f\"Created output gender folder: {current_gender_output_folder}\")\n\n        print(f\"\\nProcessing images in: {current_gender_input_folder}\")\n        for filename in tqdm(os.listdir(current_gender_input_folder), desc=f\"Processing {gender} images\"):\n            if os.path.isdir(os.path.join(current_gender_input_folder, filename)):\n                continue\n\n            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n                image_path = os.path.join(current_gender_input_folder, filename)\n\n                image = cv2.imread(image_path)\n                if image is None:\n                    print(f\"Warning: Could not read image {filename}. Skipping.\")\n                    continue\n\n                original_image = image.copy()\n\n                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n                normalized_gray = gray.astype(np.float32) / 255.0\n                blurred_image = cv2.GaussianBlur(normalized_gray, (5, 5), 0)\n                laplacian = cv2.Laplacian(blurred_image, cv2.CV_32F)\n                sharpened_image = cv2.normalize(laplacian, None, 0, 1, cv2.NORM_MINMAX)\n                preprocessed_image = np.uint8(sharpened_image * 255)\n                preprocessed_image_bgr = cv2.cvtColor(preprocessed_image, cv2.COLOR_GRAY2BGR)\n\n                h, w = preprocessed_image_bgr.shape[:2]\n                blob = cv2.dnn.blobFromImage(cv2.resize(preprocessed_image_bgr, (300, 300)), 1.0,\n                                             (300, 300), (104.0, 177.0, 123.0), False, False)\n                net.setInput(blob)\n                detections = net.forward()\n\n                max_confidence_above_threshold = -1\n                best_bbox_above_threshold = None\n                max_overall_confidence = -1\n                best_overall_bbox = None\n\n                for i in range(0, detections.shape[2]):\n                    confidence = detections[0, 0, i, 2]\n                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n                    current_bbox = tuple(box.astype(\"int\"))\n\n                    if confidence > 0.5:\n                        if confidence > max_confidence_above_threshold:\n                            max_confidence_above_threshold = confidence\n                            best_bbox_above_threshold = current_bbox\n\n                    if confidence > max_overall_confidence:\n                        max_overall_confidence = confidence\n                        best_overall_bbox = current_bbox\n\n                final_bbox = None\n                if best_bbox_above_threshold is not None:\n                    final_bbox = best_bbox_above_threshold\n                elif best_overall_bbox is not None:\n                    final_bbox = best_overall_bbox\n\n                output_filename = f\"preprocessed_face_{filename}\"\n                output_path = os.path.join(current_gender_output_folder, output_filename)\n\n                if final_bbox:\n                    (startX, startY, endX, endY) = final_bbox\n                    startX = max(0, startX)\n                    startY = max(0, startY)\n                    endX = min(w, endX)\n                    endY = min(h, endY)\n\n                    cropped_face = original_image[startY:endY, startX:endX]\n\n                    if cropped_face.size == 0:\n                        cropped_face = np.zeros((224, 224, 3), dtype=np.uint8)\n                    cv2.imwrite(output_path, cropped_face)\n                else:\n                    dummy_image = np.zeros((224, 224, 3), dtype=np.uint8)\n                    cv2.imwrite(output_path, dummy_image)\n\n# --- 0. Squeeze-and-Excitation (SE) Block Implementation ---\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c = x.size()\n        y = self.avg_pool(x.unsqueeze(-1)).squeeze(-1)\n        y = self.fc(y).view(b, c)\n        return x * y.expand_as(x)\n\n# --- 0. Focal Loss Implementation ---\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean', epsilon=1e-12, label_smoothing=0.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.epsilon = epsilon\n        self.label_smoothing = label_smoothing\n\n        if self.alpha is not None:\n            if not isinstance(self.alpha, torch.Tensor):\n                self.alpha = torch.tensor(self.alpha, dtype=torch.float32)\n\n    def forward(self, inputs, targets):\n        num_classes = inputs.shape[1]\n        \n        if self.label_smoothing > 0:\n            smoothed_targets = torch.full_like(inputs, self.label_smoothing / (num_classes - 1))\n            smoothed_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n        else:\n            smoothed_targets = F.one_hot(targets, num_classes=num_classes).float()\n\n        log_pt = F.log_softmax(inputs, dim=1)\n        pt = torch.exp(log_pt)\n\n        pt_true_class = pt.gather(1, targets.long().unsqueeze(1)).squeeze()\n\n        base_loss = -(smoothed_targets * log_pt).sum(dim=1)\n        \n        focal_term = (1 - pt_true_class).pow(self.gamma)\n        loss = focal_term * base_loss\n\n        if self.alpha is not None:\n            if self.alpha.device != inputs.device:\n                self.alpha = self.alpha.to(inputs.device)\n            alpha_t = self.alpha.gather(0, targets.long())\n            loss = alpha_t * loss\n        \n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss\n\n# --- Enhanced Supervised Contrastive Loss with Class Reweighting and Hard Negative Mining ---\nclass EnhancedSupConLoss(nn.Module):\n    def __init__(self, temperature=0.05, base_temperature=0.07, contrast_mode='all', \n                 hard_mining_ratio=0.35, margin=0.2):\n        super(EnhancedSupConLoss, self).__init__()\n        self.temperature = temperature\n        self.base_temperature = base_temperature\n        self.contrast_mode = contrast_mode\n        self.hard_mining_ratio = hard_mining_ratio\n        self.margin = margin\n        \n    def forward(self, features, labels=None, mask=None, class_weights=None):                \n        device = features.device\n\n        if len(features.shape) < 3:\n            features = features.unsqueeze(1)\n        \n        batch_size = features.shape[0]\n        original_n_views = features.shape[1]\n\n        if self.contrast_mode == 'one':\n            if original_n_views < 2:\n                raise ValueError(\"`contrast_mode='one'` requires at least 2 views (e.g., [bsz, 2, feature_dim])\")\n            anchor_feature = features[:, 0]\n            contrast_feature = features[:, 1]\n        elif self.contrast_mode == 'all':\n            anchor_feature = features.view(-1, features.shape[-1])\n            contrast_feature = anchor_feature\n            \n            if labels is not None:\n                labels = labels.repeat_interleave(original_n_views)\n                \n            batch_size = anchor_feature.shape[0]\n        else:\n            raise ValueError('Unknown contrast mode: {}'.format(self.contrast_mode))\n            \n        anchor_feature = F.normalize(anchor_feature, dim=1)\n        contrast_feature = F.normalize(contrast_feature, dim=1)\n\n        anchor_dot_contrast = torch.div(\n            torch.matmul(anchor_feature, contrast_feature.T),\n            self.temperature\n        )\n        \n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        anchor_dot_contrast = anchor_dot_contrast - logits_max.detach()\n        \n        logits_mask = 1 - torch.eye(batch_size, device=device) if self.contrast_mode == 'all' else torch.ones(batch_size, batch_size, device=device)\n        \n        if mask is None: \n            mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float().to(device)\n        \n        neg_mask = (1 - mask) * logits_mask\n        \n        if self.margin > 0:\n            margined_neg_logits = (anchor_dot_contrast * neg_mask) - (self.margin * neg_mask)\n            anchor_dot_contrast = (anchor_dot_contrast * mask) + margined_neg_logits\n        \n        if self.hard_mining_ratio < 1.0 and self.hard_mining_ratio > 0:\n            k = int(batch_size * self.hard_mining_ratio)\n            k = max(k, 1)\n\n            current_logits_mask = mask.clone()\n\n            for i in range(batch_size):\n                valid_neg_indices = torch.where(neg_mask[i] > 0)[0]\n\n                if len(valid_neg_indices) > 0:\n                    neg_sims = anchor_dot_contrast[i, valid_neg_indices]\n                    k_actual = min(k, len(valid_neg_indices))\n                    if k_actual > 0:\n                        _, hard_neg_local_indices = torch.topk(neg_sims, k_actual)\n                        hard_neg_global_indices = valid_neg_indices[hard_neg_local_indices]\n                        current_logits_mask[i, hard_neg_global_indices] = 1.0\n                \n            logits_mask = current_logits_mask\n        \n        exp_logits = torch.exp(anchor_dot_contrast) * logits_mask\n        log_prob = anchor_dot_contrast - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n        \n        positive_log_probs = log_prob * mask\n\n        sum_positive_log_probs = positive_log_probs.sum(1)\n        count_positive_pairs = mask.sum(1) + 1e-12\n\n        if class_weights is not None and labels is not None:\n            if self.contrast_mode == 'all':\n                original_labels_for_weights = labels[::original_n_views]\n            else:\n                original_labels_for_weights = labels\n\n            weight_values = torch.tensor([class_weights.get(label.item(), 1.0)\n                                          for label in original_labels_for_weights], device=device)\n            \n            if self.contrast_mode == 'all':\n                weight_values = weight_values.repeat_interleave(original_n_views)\n\n            weighted_mean_log_prob_pos = (sum_positive_log_probs * weight_values) / count_positive_pairs\n        else:\n            weighted_mean_log_prob_pos = sum_positive_log_probs / count_positive_pairs\n        \n        loss = -(self.temperature / self.base_temperature) * weighted_mean_log_prob_pos\n        loss = loss.mean()\n        \n        return loss\n\n# --- 1. Custom Dataset for Multi-View Augmentation and Combined Images ---\nclass GenderDataset(Dataset):\n    def __init__(self, data_dir, preprocessed_data_dir, transform=None, is_train=True):\n        self.data_dir = data_dir\n        self.preprocessed_data_dir = preprocessed_data_dir\n        self.transform = transform\n        self.is_train = is_train\n        self.image_paths = []\n        self.preprocessed_image_paths = []\n        self.labels = [] # 0 for female, 1 for male (consistent mapping)\n        self.class_to_idx = {'female': 0, 'male': 1}\n        self.idx_to_class = {0: 'female', 1: 'male'}\n        \n        print(f\"Loading dataset from: {data_dir} (is_train={is_train})\")\n\n        temp_image_paths = []\n        temp_preprocessed_image_paths = []\n        temp_labels = []\n\n        for gender in ['female', 'male']:\n            gender_path = os.path.join(data_dir, gender)\n            preprocessed_gender_path = os.path.join(preprocessed_data_dir, gender)\n            class_idx = self.class_to_idx[gender]\n            \n            if not os.path.exists(gender_path):\n                print(f\"Warning: Directory not found: {gender_path}. Skipping.\")\n                continue\n            if not os.path.exists(preprocessed_gender_path):\n                print(f\"Warning: Directory not found: {preprocessed_gender_path}. Skipping.\")\n                continue\n\n            for img_name in os.listdir(gender_path):\n                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n                    original_img_path = os.path.join(gender_path, img_name)\n                    preprocessed_img_name = f\"preprocessed_face_{img_name}\"\n                    preprocessed_img_path = os.path.join(preprocessed_gender_path, preprocessed_img_name)\n\n                    if os.path.exists(preprocessed_img_path):\n                        temp_image_paths.append(original_img_path)\n                        temp_preprocessed_image_paths.append(preprocessed_img_path)\n                        temp_labels.append(class_idx)\n                    else:\n                        print(f\"Warning: Corresponding preprocessed image not found for {original_img_path}. Skipping.\")\n        \n        self.image_paths = temp_image_paths\n        self.preprocessed_image_paths = temp_preprocessed_image_paths\n        self.labels = temp_labels\n\n        self.class_counts = Counter(self.labels)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        preprocessed_img_path = self.preprocessed_image_paths[idx]\n        label = self.labels[idx]\n        \n        try:\n            img = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading original image {img_path}: {e}\")\n            img = Image.new('RGB', (224, 224), color='black')\n\n        try:\n            preprocessed_img = Image.open(preprocessed_img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading preprocessed image {preprocessed_img_path}: {e}\")\n            preprocessed_img = Image.new('RGB', (224, 224), color='black')\n\n        if self.transform:\n            if self.is_train:\n                img1_original = self.transform(img)\n                img2_original = self.transform(img)\n\n                img1_processed = self.transform(preprocessed_img)\n                img2_processed = self.transform(preprocessed_img)\n                \n                return (img1_original, img2_original, img1_processed, img2_processed), label\n            else:\n                img_transformed = self.transform(img)\n                preprocessed_img_transformed = self.transform(preprocessed_img)\n                return (img_transformed, preprocessed_img_transformed), label\n        \n        return (img, preprocessed_img), label\n\n# --- 2. PyTorch Lightning DataModule ---\nclass GenderDataModule(pl.LightningDataModule):\n    def __init__(self, train_data_dir, val_data_dir, train_preprocessed_data_dir, val_preprocessed_data_dir, \n                 batch_size=64, num_workers=4, image_size=(224, 224), test_data_dir=None, test_preprocessed_data_dir=None):\n        super().__init__()\n        self.train_data_dir = train_data_dir\n        self.val_data_dir = val_data_dir\n        self.train_preprocessed_data_dir = train_preprocessed_data_dir\n        self.val_preprocessed_data_dir = val_preprocessed_data_dir\n        self.test_data_dir = test_data_dir\n        self.test_preprocessed_data_dir = test_preprocessed_data_dir\n\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.image_size = image_size\n\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n        self.train_transform = transforms.Compose([\n            transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0), ratio=(0.75, 1.33)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n            transforms.RandomRotation(degrees=20),\n            transforms.GaussianBlur(kernel_size=3),\n            transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n            transforms.ToTensor(),\n            self.normalize,\n            transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3))\n        ])\n        \n        self.val_transform = transforms.Compose([\n            transforms.Resize(int(image_size[0] / 0.875)),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            self.normalize\n        ])\n\n        self.train_dataset = None\n        self.val_dataset = None\n        self.test_dataset = None\n        self.class_weights_for_loss = None\n        self.class_weights_tensor = None\n\n    def setup(self, stage=None):\n        if stage == 'fit' or stage is None:\n            self.train_dataset = GenderDataset(self.train_data_dir, self.train_preprocessed_data_dir, transform=self.train_transform, is_train=True)\n            self.val_dataset = GenderDataset(self.val_data_dir, self.val_preprocessed_data_dir, \n                                             transform=self.val_transform, is_train=False)\n\n            total_samples = len(self.train_dataset)\n            num_classes = len(self.train_dataset.class_to_idx)\n            \n            female_count = self.train_dataset.class_counts.get(0, 0)\n            male_count = self.train_dataset.class_counts.get(1, 0)\n\n            weight_female = total_samples / (num_classes * female_count) if female_count > 0 else 1.0\n            weight_male = total_samples / (num_classes * male_count) if male_count > 0 else 1.0\n            \n            self.class_weights_for_loss = {0: weight_female, 1: weight_male}\n            self.class_weights_tensor = torch.tensor([weight_female, weight_male], dtype=torch.float32)\n\n            print(f\"Calculated class weights for loss (Female: {self.class_weights_for_loss[0]:.2f}, Male: {self.class_weights_for_loss[1]:.2f})\")\n            print(f\"Train dataset class counts: {self.train_dataset.class_counts}\")\n\n        if stage == 'test' or stage is None:\n            if self.test_data_dir and self.test_preprocessed_data_dir:\n                self.test_dataset = GenderDataset(self.test_data_dir, self.test_preprocessed_data_dir,\n                                                  transform=self.val_transform, is_train=False)\n\n    def train_dataloader(self):\n        labels = self.train_dataset.labels\n        sample_weights = [self.class_weights_for_loss[label] for label in labels]\n        \n        sample_weights = np.array(sample_weights)\n        sample_weights[~np.isfinite(sample_weights)] = 1.0\n\n        sampler = WeightedRandomSampler(\n            weights=list(sample_weights),\n            num_samples=len(sample_weights),\n            replacement=True\n        )\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n    def test_dataloader(self):\n        if self.test_dataset:\n            return DataLoader(\n                self.test_dataset,\n                batch_size=self.batch_size,\n                shuffle=False,\n                num_workers=self.num_workers,\n                pin_memory=True\n            )\n        else:\n            raise RuntimeError(\"Test dataset not set up. Call setup('test') first.\")\n\n\n# --- 3. PyTorch Lightning Model with EfficientNetB3 and EnhancedSupConLoss ---\nclass GenderClassificationModel(pl.LightningModule):\n    def __init__(self, num_classes=2, learning_rate=1e-4, weight_decay=1e-5, \n                 supcon_temp=0.07, supcon_base_temp=0.07, supcon_hard_mining_ratio=0.35, \n                 supcon_margin=0.2, class_weights_for_loss=None, class_weights_tensor=None,\n                 max_epochs: int = 50, label_smoothing: float = 0.0, gamma: float = 2.0):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.feature_extractor_original = timm.create_model('efficientnet_b3', pretrained=True, num_classes=0)\n        efficientnet_feature_dim = self.feature_extractor_original.num_features \n\n        self.feature_extractor_processed = timm.create_model('efficientnet_b3', pretrained=True, num_classes=0)\n        \n        combined_feature_dim = efficientnet_feature_dim * 2\n\n        self.se_block = SEBlock(channel=combined_feature_dim)\n        \n        self.loss_weight_param = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))\n\n        self.projection_head = nn.Sequential(\n            nn.Linear(combined_feature_dim, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 128)\n        )\n        \n        self.classification_head = nn.Sequential(\n            nn.Linear(combined_feature_dim, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n\n        self.supcon_loss_fn = EnhancedSupConLoss(\n            temperature=supcon_temp,\n            base_temperature=supcon_base_temp,\n            contrast_mode='all',\n            hard_mining_ratio=supcon_hard_mining_ratio, \n            margin=supcon_margin \n        )\n        \n        if class_weights_tensor is not None:\n            alpha_for_focal = torch.tensor([class_weights_for_loss[0], class_weights_for_loss[1]], dtype=torch.float32)\n        else:\n            alpha_for_focal = None\n\n        self.focal_loss_fn = FocalLoss(\n            alpha=alpha_for_focal,\n            gamma=self.hparams.gamma,\n            label_smoothing=label_smoothing\n        )\n        \n        self.class_weights_for_loss = class_weights_for_loss\n        self.class_weights_tensor = class_weights_tensor\n\n        self.train_raw_preds = []\n        self.train_labels = []\n        self.val_raw_preds = []\n        self.val_labels = []\n        self.test_raw_preds = []\n        self.test_labels = []\n\n\n    def forward(self, x):\n        if isinstance(x, tuple) and len(x) == 4:\n            img1_original, img2_original, img1_processed, img2_processed = x\n\n            features1_original = self.feature_extractor_original(img1_original)\n            features2_original = self.feature_extractor_original(img2_original)\n            features1_processed = self.feature_extractor_processed(img1_processed)\n            features2_processed = self.feature_extractor_processed(img2_processed)\n\n            features1_combined = torch.cat((features1_original, features1_processed), dim=1)\n            features2_combined = torch.cat((features2_original, features2_processed), dim=1)\n            \n            features1_combined_se = self.se_block(features1_combined)\n            features2_combined_se = self.se_block(features2_combined)\n\n            proj_features1 = self.projection_head(features1_combined_se)\n            proj_features2 = self.projection_head(features2_combined_se)\n            \n            supcon_features = torch.stack((proj_features1, proj_features2), dim=1)\n            \n            logits = self.classification_head(features1_combined_se) \n            return supcon_features, logits\n        elif isinstance(x, tuple) and len(x) == 2:\n            img_original, img_processed = x\n            features_original = self.feature_extractor_original(img_original)\n            features_processed = self.feature_extractor_processed(img_processed)\n            \n            features_combined = torch.cat((features_original, features_processed), dim=1)\n            \n            features_combined_se = self.se_block(features_combined)\n            \n            logits = self.classification_head(features_combined_se)\n            return logits\n        else:\n            raise ValueError(\"Unexpected input format for forward pass. Expected tuple of 2 or 4 tensors.\")\n\n\n    def training_step(self, batch, batch_idx):\n        (imgs1_original, imgs2_original, imgs1_processed, imgs2_processed), labels = batch\n        \n        supcon_features, logits = self((imgs1_original, imgs2_original, imgs1_processed, imgs2_processed))\n\n        supcon_loss = self.supcon_loss_fn(\n            features=supcon_features,\n            labels=labels,\n            class_weights=self.class_weights_for_loss\n        )\n\n        ce_loss = self.focal_loss_fn(logits, labels)\n        \n        total_loss = self.loss_weight_param * supcon_loss + (1 - self.loss_weight_param) * ce_loss\n        \n        self.log('train_supcon_loss', supcon_loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_ce_loss', ce_loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('loss_weight_param', self.loss_weight_param, on_step=True, on_epoch=True, prog_bar=True)\n\n\n        self.train_raw_preds.append(F.softmax(logits, dim=1).detach().cpu().numpy())\n        self.train_labels.append(labels.cpu().numpy())\n        \n        return total_loss\n\n    def validation_step(self, batch, batch_idx):\n        (imgs_original, imgs_processed), labels = batch\n        logits = self((imgs_original, imgs_processed))\n        \n        loss = self.focal_loss_fn(logits, labels)\n        \n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.val_raw_preds.append(F.softmax(logits, dim=1).detach().cpu().numpy())\n        self.val_labels.append(labels.cpu().numpy())\n        \n        return loss\n\n    def test_step(self, batch, batch_idx):\n        (imgs_original, imgs_processed), labels = batch\n        logits = self((imgs_original, imgs_processed))\n        \n        loss = self.focal_loss_fn(logits, labels)\n        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.test_raw_preds.append(F.softmax(logits, dim=1).detach().cpu().numpy())\n        self.test_labels.append(labels.cpu().numpy())\n        \n        return loss\n\n    def on_train_epoch_end(self):\n        if len(self.train_raw_preds) == 0:\n            return\n        \n        all_raw_preds = np.concatenate(self.train_raw_preds)\n        all_labels = np.concatenate(self.train_labels)\n        \n        all_preds_classes = np.argmax(all_raw_preds, axis=1)\n\n        accuracy = accuracy_score(all_labels, all_preds_classes)\n        self.log('train_accuracy_epoch', accuracy, prog_bar=True)\n\n        f1 = f1_score(all_labels, all_preds_classes, average='weighted', zero_division=0)\n        self.log('train_f1_epoch', f1, prog_bar=True)\n\n        try:\n            if len(np.unique(all_labels)) > 1:\n                roc_auc = roc_auc_score(all_labels, all_raw_preds[:, 1], average='weighted')\n                self.log('train_roc_auc_epoch', roc_auc, prog_bar=True)\n            else:\n                self.log('train_roc_auc_epoch', 0.0, prog_bar=True)\n        except ValueError:\n            self.log('train_roc_auc_epoch', 0.0, prog_bar=True)\n\n        try:\n            if len(np.unique(all_labels)) > 1:\n                precision, recall, _ = precision_recall_curve(all_labels, all_raw_preds[:, 1])\n                pr_auc = auc(recall, precision)\n                self.log('train_pr_auc_epoch', pr_auc, prog_bar=True)\n            else:\n                self.log('train_pr_auc_epoch', 0.0, prog_bar=True)\n        except ValueError:\n            self.log('train_pr_auc_epoch', 0.0, prog_bar=True)\n        \n        self.train_raw_preds.clear()\n        self.train_labels.clear()\n\n    def on_validation_epoch_end(self):\n        if len(self.val_raw_preds) == 0:\n            return\n        \n        all_raw_preds = np.concatenate(self.val_raw_preds)\n        all_labels = np.concatenate(self.val_labels)\n        \n        all_preds_classes = np.argmax(all_raw_preds, axis=1)\n\n        accuracy = accuracy_score(all_labels, all_preds_classes)\n        self.log('val_accuracy_epoch', accuracy, prog_bar=True)\n\n        f1 = f1_score(all_labels, all_preds_classes, average='weighted', zero_division=0)\n        self.log('val_f1_epoch', f1, prog_bar=True)\n\n        try:\n            if len(np.unique(all_labels)) > 1:\n                roc_auc = roc_auc_score(all_labels, all_raw_preds[:, 1], average='weighted')\n                self.log('val_roc_auc_epoch', roc_auc, prog_bar=True)\n            else:\n                self.log('val_roc_auc_epoch', 0.0, prog_bar=True)\n        except ValueError:\n            self.log('val_roc_auc_epoch', 0.0, prog_bar=True)\n\n        try:\n            if len(np.unique(all_labels)) > 1:\n                precision, recall, _ = precision_recall_curve(all_labels, all_raw_preds[:, 1])\n                pr_auc = auc(recall, precision)\n                self.log('val_pr_auc_epoch', pr_auc, prog_bar=True)\n            else:\n                self.log('val_pr_auc_epoch', 0.0, prog_bar=True)\n        except ValueError:\n            self.log('val_pr_auc_epoch', 0.0, prog_bar=True)\n        \n        self.val_raw_preds.clear()\n        self.val_labels.clear()\n\n    def on_test_epoch_end(self):\n        if len(self.test_raw_preds) == 0:\n            print(\"No test predictions collected.\")\n            return\n        \n        all_raw_preds = np.concatenate(self.test_raw_preds)\n        all_labels = np.concatenate(self.test_labels)\n        \n        all_preds_classes = np.argmax(all_raw_preds, axis=1)\n\n        print(\"\\n--- Comprehensive Evaluation Report (Test Set) ---\")\n        accuracy_test = accuracy_score(all_labels, all_preds_classes)\n        print(f\"Overall Accuracy (Test Set): {accuracy_test:.4f}\")\n        print(\"\\nDetailed Classification Report (Test Set):\")\n        class_names = ['female', 'male'] \n        print(classification_report(all_labels, all_preds_classes, target_names=class_names, zero_division=0))\n\n        print(\"\\n--- Confusion Matrix (Test Set) ---\")\n        cm_test = confusion_matrix(all_labels, all_preds_classes)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.title('Confusion Matrix (Test Set)')\n        plt.show()\n\n        try:\n            if len(np.unique(all_labels)) > 1:\n                roc_auc_test = roc_auc_score(all_labels, all_raw_preds[:, 1], average='weighted')\n                print(f\"Final Test ROC-AUC (weighted): {roc_auc_test:.4f}\")\n            else:\n                print(\"Cannot compute ROC-AUC: Only one class present in test labels.\")\n        except Exception as e:\n            print(f\"Error computing final Test ROC-AUC: {e}\")\n\n        try:\n            if len(np.unique(all_labels)) > 1:\n                precision_test, recall_test, _ = precision_recall_curve(all_labels, all_raw_preds[:, 1])\n                pr_auc_test = auc(recall_test, precision_test)\n                print(f\"Final Test PR-AUC (weighted): {pr_auc_test:.4f}\")\n            else:\n                print(\"Cannot compute PR-AUC: Only one class present in test labels.\")\n        except Exception as e:\n            print(f\"Error computing final Test PR-auc: {e}\")\n        \n        self.test_raw_preds.clear()\n        self.test_labels.clear()\n\n    def configure_optimizers(self):\n        optimizer_params = [\n            {'params': self.feature_extractor_original.parameters(), 'lr': self.hparams.learning_rate},\n            {'params': self.feature_extractor_processed.parameters(), 'lr': self.hparams.learning_rate},\n            {'params': self.se_block.parameters(), 'lr': self.hparams.learning_rate},\n            {'params': self.projection_head.parameters(), 'lr': self.hparams.learning_rate * 2},\n            {'params': self.classification_head.parameters(), 'lr': self.hparams.learning_rate * 2},\n            {'params': self.loss_weight_param, 'lr': self.hparams.learning_rate * 5}\n        ]\n\n        optimizer = torch.optim.AdamW(optimizer_params, weight_decay=self.hparams.weight_decay)\n        \n        scheduler = {\n            'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                optimizer,\n                T_0=self.hparams.max_epochs // 5,\n                T_mult=2,\n                eta_min=self.hparams.learning_rate / 100,\n                verbose=True\n            ),\n            'monitor': 'val_loss',\n            'interval': 'epoch',\n            'frequency': 1\n        }\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n# --- Utility Function for Visualizing Misclassifications ---\ndef visualize_misclassifications(model, dataloader, class_names, num_images=None):\n    \"\"\"\n    Visualizes misclassified images with their true and predicted labels.\n    If num_images is None, all misclassified images are shown.\n    \"\"\"\n    misclassified_samples = []\n    \n    model.eval()\n    device = next(model.parameters()).device\n\n    with torch.no_grad():\n        for batch_idx, (imgs_tuple, labels) in enumerate(dataloader):\n            imgs_original, imgs_processed = imgs_tuple\n            \n            imgs_original = imgs_original.to(device)\n            imgs_processed = imgs_processed.to(device)\n            labels = labels.to(device)\n\n            logits = model((imgs_original, imgs_processed))\n            predicted_probs = F.softmax(logits, dim=1)\n            predicted_labels = torch.argmax(predicted_probs, dim=1)\n\n            for i in range(len(labels)):\n                if predicted_labels[i] != labels[i]:\n                    img_cpu = imgs_original[i].cpu() \n                    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n                    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n                    img_display = img_cpu * std + mean\n                    img_display = torch.clamp(img_display, 0, 1)\n                    \n                    misclassified_samples.append({\n                        'image': img_display,\n                        'true_label': class_names[labels[i].item()],\n                        'predicted_label': class_names[predicted_labels[i].item()]\n                    })\n            if num_images is not None and len(misclassified_samples) >= num_images:\n                break\n    \n    if misclassified_samples:\n        display_count = len(misclassified_samples) if num_images is None else min(num_images, len(misclassified_samples))\n        print(f\"\\n--- Visualizing {display_count} Misclassified Images ---\")\n        \n        cols = 5\n        rows = (display_count + cols - 1) // cols\n        \n        plt.figure(figsize=(15, 4 * rows))\n        for i, sample in enumerate(misclassified_samples[:display_count]):\n            plt.subplot(rows, cols, i + 1)\n            plt.imshow(sample['image'].permute(1, 2, 0).numpy())\n            plt.title(f\"True: {sample['true_label']}\\nPred: {sample['predicted_label']}\")\n            plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"\\nNo misclassified images found in the set provided for visualization!\")\n\n# --- Main Test Script ---\nif __name__ == '__main__':\n    # Hyperparameters (must match those used for training the checkpoint)\n    BATCH_SIZE = 16\n    NUM_WORKERS = 4\n    LEARNING_RATE = 5e-5\n    WEIGHT_DECAY = 1e-5\n    MAX_EPOCHS = 50 \n    LABEL_SMOOTHING_EPSILON = 0.1\n    FOCAL_LOSS_GAMMA = 3.0\n\n    SUPCON_TEMP = 0.07 \n    SUPCON_BASE_TEMP = 0.07 \n    SUPCON_HARD_MINING_RATIO = 0.5\n    SUPCON_MARGIN = 0.3\n\n    VISUALIZE_ALL_MISCLASSIFICATIONS = True \n\n    # --- Step 1: Set up face detection model files (if not already present) ---\n    print(\"--- Setting up face detection model files ---\")\n    setup_face_detection_models()\n\n    # --- Step 2: Preprocess the Test Dataset ---\n    print(f\"\\n--- Preprocessing Test Dataset from {TEST_INPUT_PATH} to {TEST_PREPROCESSED_OUTPUT_DIR} ---\")\n    preprocess_and_detect_face(TEST_INPUT_PATH, TEST_PREPROCESSED_OUTPUT_DIR)\n    print(\"Test dataset preprocessing complete.\")\n\n    # --- Step 3: Initialize DataModule for Test Set ---\n    data_module = GenderDataModule(\n        train_data_dir=None,\n        val_data_dir=None,\n        train_preprocessed_data_dir=None,\n        val_preprocessed_data_dir=None,\n        test_data_dir=TEST_INPUT_PATH,\n        test_preprocessed_data_dir=TEST_PREPROCESSED_OUTPUT_DIR, # <--- MODIFIED HERE: Using the new explicit path\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n    )\n    data_module.setup('test')\n\n    if data_module.test_dataset is None or len(data_module.test_dataset) == 0:\n        print(\"Error: Test dataset is empty or not loaded properly. Cannot proceed with evaluation.\")\n    else:\n        # --- Step 4: Load the Trained Model from Checkpoint ---\n        print(f\"\\n--- Loading model from checkpoint: {CHECKPOINT_PATH} ---\")\n        try:\n            model = GenderClassificationModel.load_from_checkpoint(\n                CHECKPOINT_PATH,\n                map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n                num_classes=2,\n                learning_rate=LEARNING_RATE,\n                weight_decay=WEIGHT_DECAY,\n                supcon_temp=SUPCON_TEMP,\n                supcon_base_temp=SUPCON_BASE_TEMP,\n                supcon_hard_mining_ratio=SUPCON_HARD_MINING_RATIO,\n                supcon_margin=SUPCON_MARGIN,\n                class_weights_for_loss=None,\n                class_weights_tensor=None,\n                max_epochs=MAX_EPOCHS,\n                label_smoothing=LABEL_SMOOTHING_EPSILON,\n                gamma=FOCAL_LOSS_GAMMA\n            )\n            model.eval()\n            model.freeze()\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            model.to(device)\n            print(\"Model loaded successfully.\")\n\n            # --- Step 5: Perform Evaluation on Test Set ---\n            print(\"\\n--- Running Evaluation on Test Set ---\")\n            trainer = pl.Trainer(\n                accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n                devices=1,\n                precision=16 if torch.cuda.is_available() else 32,\n                logger=False\n            )\n            \n            trainer.test(model, dataloaders=data_module.test_dataloader())\n\n            # --- Step 6: Visualize Misclassifications on Test Set ---\n            print(\"\\n--- Visualizing Misclassifications on Test Set ---\")\n            class_names = list(data_module.test_dataset.idx_to_class.values())\n            num_images_to_viz = None if VISUALIZE_ALL_MISCLASSIFICATIONS else 10\n            visualize_misclassifications(\n                model, \n                data_module.test_dataloader(), \n                class_names, \n                num_images=num_images_to_viz\n            )\n\n        except FileNotFoundError:\n            print(f\"Error: Checkpoint file not found at {CHECKPOINT_PATH}. Please ensure the path is correct and the file exists.\")\n        except Exception as e:\n            print(f\"An error occurred during model loading or testing: {e}\")\n\n    print(\"\\nTest pipeline execution complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}