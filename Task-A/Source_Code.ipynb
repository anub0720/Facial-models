{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12196727,"sourceType":"datasetVersion","datasetId":7682854},{"sourceId":12198873,"sourceType":"datasetVersion","datasetId":7683237},{"sourceId":12202391,"sourceType":"datasetVersion","datasetId":7686443},{"sourceId":12224203,"sourceType":"datasetVersion","datasetId":7701565},{"sourceId":12227476,"sourceType":"datasetVersion","datasetId":7703701}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n      #  print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-20T12:41:55.828800Z","iopub.execute_input":"2025-06-20T12:41:55.829596Z","iopub.status.idle":"2025-06-20T12:41:55.833619Z","shell.execute_reply.started":"2025-06-20T12:41:55.829570Z","shell.execute_reply":"2025-06-20T12:41:55.832859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ§¹ Cleanup Script for Kaggle Working Directory\n\nThis script removes **all directories** inside the specified working directory (`/kaggle/working/`).\n\n### Workflow:\n\n1. Lists all items in the `working_dir`.\n2. Iterates through each item:\n   - If it is a **directory**, attempts to delete it and prints confirmation.\n   - Handles any errors gracefully, printing an error message if removal fails.\n3. (Optional) You can enable file removal by uncommenting the corresponding block.\n\n### Usage:\n\n- Keeps the Kaggle working directory clean by removing residual directories between runs.\n- Helps avoid clutter or conflicts from previous outputs or temporary data.\n\n---\n\n**Note:**  \nBe cautious when enabling file removal to avoid accidental deletion of important files.\n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the working directory\nworking_dir = '/kaggle/working/'\n\n# Get a list of all items in the working directory\nitems_in_working = os.listdir(working_dir)\n\n# Iterate through the items and remove directories\nprint(f\"Cleaning up {working_dir}...\")\nfor item in items_in_working:\n    item_path = os.path.join(working_dir, item)\n    if os.path.isdir(item_path):\n        try:\n            shutil.rmtree(item_path)\n            print(f\"Removed directory: {item_path}\")\n        except OSError as e:\n            print(f\"Error removing directory {item_path}: {e}\")\n    # Optional: If you also want to remove files, uncomment the else if below\n    # elif os.path.isfile(item_path):\n    #     try:\n    #         os.remove(item_path)\n    #         print(f\"Removed file: {item_path}\")\n    #     except OSError as e:\n    #         print(f\"Error removing file {item_path}: {e}\")\n\nprint(\"Cleanup complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“¦ Install OpenCV-Python\n\nTo install the OpenCV Python package, run:\n\n```bash\npip install opencv-python\n","metadata":{}},{"cell_type":"code","source":"pip install opencv-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“¥ Download Face Detection Model Files\n\nThis snippet performs the following:\n\n1. **Creates a directory** at `/kaggle/working/face-detection-model-files/` if it does not already exist.\n2. **Downloads two essential files** for OpenCVâ€™s DNN face detector into that directory:\n   - `deploy.prototxt`: The model architecture definition.\n   - `res10_300x300_ssd_iter_140000.caffemodel`: The pre-trained weights.\n\n---\n\n### Bash commands used inside Python with `wget`:\n\n```bash\nwget -P /kaggle/working/face-detection-model-files/ https://raw.githubusercontent.com/opencv/opencv/4.x/samples/dnn/face_detector/deploy.prototxt\nwget -P /kaggle/working/face-detection-model-files/ https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\n","metadata":{}},{"cell_type":"code","source":"import os\n\n# Create the directory if it doesn't exist\nmodel_dir = \"/kaggle/working/face-detection-model-files/\"\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n    print(f\"Created directory: {model_dir}\")\n\n# Download deploy.prototxt\n!wget -P {model_dir} https://raw.githubusercontent.com/opencv/opencv/4.x/samples/dnn/face_detector/deploy.prototxt\n\n# Download res10_300x300_ssd_iter_140000.caffemodel\n!wget -P {model_dir} https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Face Preprocessing and Single-Face Detection Pipeline\n\nThis script preprocesses images from an input directory, detects a single face using a pretrained Caffe SSD model, crops the detected face, and saves it to an output directory. If no high-confidence face is found, it selects the detection with the highest confidence. If no face is detected at all, a dummy black image is saved as a placeholder.\n\n---\n\n## Features\n\n- **Image preprocessing:**  \n  - Grayscale conversion  \n  - Normalization  \n  - Gaussian blur for noise reduction  \n  - Laplacian filter for edge enhancement  \n- **Face detection:**  \n  - Uses OpenCV's DNN face detector with Caffe model (ResNet SSD)  \n  - Selects the best face based on confidence threshold (default 0.5)  \n- **Robust handling:**  \n  - If no high-confidence detection, fallback to best detection overall  \n  - Creates dummy black image if no face is detected  \n- **Batch processing:**  \n  - Processes images grouped by dataset splits (`train`, `val`) and gender (`male`, `female`)\n\n---\n\n## Dependencies\n\n- Python 3.x  \n- OpenCV (`opencv-python`)  \n- NumPy  \n\nInstall OpenCV if not installed:\n\n```bash\npip install opencv-python\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\n\ndef preprocess_and_detect_face(input_folder, output_folder):\n    \"\"\"\n    Preprocesses images, detects a single face, and saves the cropped face\n    to a new folder. If no high-confidence face is found, it selects the\n    detection with the highest overall confidence.\n\n    Args:\n        input_folder (str): Path to the folder containing input images.\n        output_folder (str): Path to the folder where preprocessed and\n                              cropped face images will be saved.\n    \"\"\"\n\n    # Create the output folder if it doesn't exist\n    try:\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n            print(f\"Created output folder: {output_folder}\")\n        else:\n            print(f\"Output folder already exists: {output_folder}\")\n    except OSError as e:\n        print(f\"Error creating output folder '{output_folder}': {e}\")\n        print(\"Please check your permissions or the specified path.\")\n        return # Exit if folder cannot be created\n\n    # --- Load pre-trained face detection model (Caffe model - ResNet SSD) ---\n    # These files are crucial for face detection.\n    prototxt_path = \"/kaggle/working/face-detection-model-files/deploy.prototxt\"\n    caffemodel_path = \"/kaggle/working/face-detection-model-files/res10_300x300_ssd_iter_140000.caffemodel\"\n\n    if not os.path.exists(prototxt_path) or not os.path.exists(caffemodel_path):\n        print(\"Error: Pre-trained Caffe model files (deploy.prototxt and res10_300x300_ssd_iter_140000.caffemodel) not found.\")\n        print(\"Please ensure they are correctly placed at the specified Kaggle input path:\")\n        print(f\"Prototxt Path: {prototxt_path}\")\n        print(f\"Caffemodel Path: {caffemodel_path}\")\n        print(\"Download links:\")\n        print(\"deploy.prototxt download: https://github.com/opencv/opencv/blob/4.x/samples/dnn/face_detector/deploy.prototxt\")\n        print(\"res10_300x300_ssd_iter_140000.caffemodel download: https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\")\n        return\n\n    net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n    print(\"Pre-trained face detection model loaded successfully.\")\n\n    # Iterate through all files in the input folder\n    if not os.path.exists(input_folder):\n        print(f\"Error: Input folder '{input_folder}' does not exist. Please check the path.\")\n        return\n        \n    for filename in os.listdir(input_folder):\n        # Skip if it's a directory\n        if os.path.isdir(os.path.join(input_folder, filename)):\n            continue\n\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n            image_path = os.path.join(input_folder, filename)\n            print(f\"\\nProcessing image: {image_path}\")\n\n            # --- Read the image ---\n            image = cv2.imread(image_path)\n            if image is None:\n                print(f\"Warning: Could not read image {filename}. Skipping.\")\n                continue\n\n            original_image = image.copy() # Keep a copy for later cropping\n\n            # --- Phase 1: Image Pre-processing for Enhanced Detection ---\n            # 1. Grayscale Conversion: Reduces computational complexity.\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n            # 2. Normalization: Standardizes pixel intensity values.\n            # Convert to float and normalize to [0, 1]\n            normalized_gray = gray.astype(np.float32) / 255.0\n            \n            # 3. Noise Reduction (Gaussian Blur): Smoothes image, removes noise.\n            # Kernel size (5,5) is a common choice, adjust as needed.\n            blurred_image = cv2.GaussianBlur(normalized_gray, (5, 5), 0)\n\n            # 4. Edge Enhancement (Laplacian Filter): Highlights boundaries.\n            # Re-normalize to 0-255 after applying filter, as Laplacian can produce negative values.\n            laplacian = cv2.Laplacian(blurred_image, cv2.CV_32F)\n            # Scale and shift to bring values into 0-1 range, then back to 0-255\n            sharpened_image = cv2.normalize(laplacian, None, 0, 1, cv2.NORM_MINMAX)\n            preprocessed_image = np.uint8(sharpened_image * 255) # Convert back to 8-bit for DNN\n\n            # IMPORTANT FIX: Convert preprocessed_image to 3-channel (BGR) as the Caffe model expects 3 channels.\n            preprocessed_image_bgr = cv2.cvtColor(preprocessed_image, cv2.COLOR_GRAY2BGR)\n\n            # --- Phase 2: Robust Single-Face Detection ---\n            # Prepare the preprocessed image for the DNN model\n            # The model expects a blob of images.\n            # Args: image, scalefactor, size, mean, swapRB, crop\n            # scalefactor: Multiplier for image values (1.0/255.0 if input is 0-255)\n            # size: Spatial size for output image (300x300 for this Caffe model)\n            # mean: Mean subtraction values (B, G, R)\n            # swapRB: Swap R and B channels (True for BGR to RGB)\n            # crop: Crop image after resizing (True)\n            h, w = preprocessed_image_bgr.shape[:2] # Get height and width from the 3-channel image\n            blob = cv2.dnn.blobFromImage(cv2.resize(preprocessed_image_bgr, (300, 300)), 1.0,\n                                         (300, 300), (104.0, 177.0, 123.0), False, False)\n            net.setInput(blob)\n            detections = net.forward()\n\n            # --- Post-processing for Single Face Extraction ---\n            # Initialize variables to store the best detection\n            max_confidence_above_threshold = -1\n            best_bbox_above_threshold = None\n\n            max_overall_confidence = -1\n            best_overall_bbox = None\n\n            # Loop over the detections to find both high-confidence and overall best\n            for i in range(0, detections.shape[2]):\n                confidence = detections[0, 0, i, 2]\n                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n                current_bbox = tuple(box.astype(\"int\"))\n\n                # Keep track of the best detection above threshold\n                if confidence > 0.5: # Confidence threshold, can be adjusted\n                    if confidence > max_confidence_above_threshold:\n                        max_confidence_above_threshold = confidence\n                        best_bbox_above_threshold = current_bbox\n                \n                # Always keep track of the overall best detection\n                if confidence > max_overall_confidence:\n                    max_overall_confidence = confidence\n                    best_overall_bbox = current_bbox\n            \n            # Determine which bounding box to use\n            final_bbox = None\n            if best_bbox_above_threshold is not None:\n                final_bbox = best_bbox_above_threshold\n                print(f\"Using high-confidence detection (confidence: {max_confidence_above_threshold:.2f}).\")\n            elif best_overall_bbox is not None:\n                final_bbox = best_overall_bbox\n                print(f\"No high-confidence face found. Using best overall detection (confidence: {max_overall_confidence:.2f}).\")\n            \n            if final_bbox:\n                (startX, startY, endX, endY) = final_bbox\n                # Ensure bounding box coordinates are within image boundaries\n                startX = max(0, startX)\n                startY = max(0, startY)\n                endX = min(w, endX)\n                endY = min(h, endY) # Corrected: Ensure endY is within height bounds.\n\n                # Crop the face from the original image (not the preprocessed one)\n                cropped_face = original_image[startY:endY, startX:endX]\n\n                if cropped_face.size == 0:\n                    print(f\"Warning: Cropped face from {filename} is empty or invalid. This might be due to bad bbox coordinates.\")\n                    print(f\"Creating a dummy black image for {filename} as a placeholder.\")\n                    cropped_face = np.zeros((100, 100, 3), dtype=np.uint8) # Create a 100x100 black image\n                \n                # Define the output path for the cropped face\n                output_filename = f\"preprocessed_face_{filename}\"\n                output_path = os.path.join(output_folder, output_filename)\n\n                # Save the cropped face image\n                cv2.imwrite(output_path, cropped_face)\n                print(f\"Saved preprocessed and cropped face to: {output_path}\")\n\n            else:\n                print(f\"No face detections found for {filename} whatsoever. Creating a dummy black image as a placeholder.\")\n                # If no detection is found at all, create a dummy black image as a placeholder.\n                dummy_image = np.zeros((100, 100, 3), dtype=np.uint8)\n                output_filename = f\"preprocessed_face_{filename}\"\n                output_path = os.path.join(output_folder, output_filename)\n                cv2.imwrite(output_path, dummy_image)\n                print(f\"Saved dummy black image to: {output_path}\")\n\n\n# --- Main execution block ---\nif __name__ == \"__main__\":\n    # Define your base input and output folders\n    # IMPORTANT: Adjust this path to your actual base input image folder on Kaggle\n    base_input_folder = \"/kaggle/input/comsys-taska/Task_A\"\n    base_output_folder = \"/kaggle/working/preprocessed_faces\" # Base output folder for all preprocessed faces\n\n    # Define the combinations of sets and genders\n    sets = [\"train\", \"val\"]\n    genders = [\"male\", \"female\"]\n\n    for data_set in sets:\n        for gender in genders:\n            current_input_folder = os.path.join(base_input_folder, data_set, gender)\n            current_output_folder = os.path.join(base_output_folder, data_set, gender)\n            \n            print(f\"\\n--- Processing {data_set}/{gender} images ---\")\n            preprocess_and_detect_face(current_input_folder, current_output_folder)\n            print(f\"--- Finished processing {data_set}/{gender} images ---\")\n\n    print(\"\\nAll processing complete. Check the specified output folder for preprocessed images.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PyTorch Lightning & EfficientNet Setup Overview\n\nThis document summarizes the key Python libraries and modules used for training an image classification model using PyTorch Lightning and EfficientNet architectures.\n\n---\n\n## Libraries and Their Roles\n\n- **os**: For handling file and directory operations.\n\n- **torch, torch.nn, torch.nn.functional**: Core PyTorch modules for tensor computations, neural network layers, and functions.\n\n- **torch.utils.data.Dataset, DataLoader, WeightedRandomSampler**: Tools for creating datasets, loading data in batches, and balancing classes during training.\n\n- **torchvision.transforms, models**: Utilities for image preprocessing, augmentation, and pretrained models.\n\n- **PIL.Image**: Image reading and manipulation.\n\n- **pytorch_lightning**: High-level framework to simplify training loops, logging, and checkpointing in PyTorch.\n\n- **pytorch_lightning.loggers.CSVLogger**: For logging training metrics into CSV files.\n\n- **pytorch_lightning.callbacks (ModelCheckpoint, EarlyStopping, LearningRateMonitor)**: Callbacks to save the best model, stop training early when performance plateaus, and monitor learning rate changes.\n\n- **numpy**: Numerical computations and array manipulations.\n\n- **collections.Counter**: For counting frequency of labels, useful in handling imbalanced datasets.\n\n- **sklearn.metrics**: Metrics for model evaluation, including F1 score, ROC AUC, precision-recall curves, classification reports, confusion matrices, and accuracy.\n\n- **warnings**: To suppress specific non-critical warnings during execution for cleaner output.\n\n- **matplotlib.pyplot, seaborn**: Visualization libraries for plotting training metrics and confusion matrices.\n\n- **tqdm**: Displays progress bars during loops for better tracking of long-running processes.\n\n- **timm**: Provides access to EfficientNet models and other state-of-the-art pretrained architectures.\n\n---\n\n## Additional Notes\n\n- The combination of these libraries enables efficient, scalable training and evaluation of deep learning models with strong support for transfer learning and model interpretability.\n\n- Class balancing is handled via `WeightedRandomSampler` to mitigate dataset imbalance.\n\n- The use of PyTorch Lightning streamlines the training pipeline, reducing boilerplate and increasing reproducibility.\n\n---\n","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, auc, classification_report, confusion_matrix, accuracy_score\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n# Import timm library for EfficientNet\nimport timm\n\n# Suppress specific warnings if needed, e.g., about PIL image conversion or small batches\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='torchvision.transforms.functional_tensor')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Modules Documentation\n\n---\n\n## 0. Squeeze-and-Excitation (SE) Block\n\nThe SE Block is designed to recalibrate channel-wise feature responses by explicitly modelling interdependencies between channels. It works by:\n\n- Applying global average pooling across the feature dimension to extract channel-wise statistics.\n- Passing these statistics through two fully connected layers with a ReLU activation in between.\n- Using a sigmoid activation to generate per-channel gating weights.\n- Multiplying the original features by these gating weights to emphasize important channels adaptively.\n\nThis mechanism helps the network focus on the most informative features, improving performance in tasks involving feature extraction.\n\n---\n\n## 0. Focal Loss\n\nFocal Loss is a loss function that addresses the problem of class imbalance by:\n\n- Down-weighting easy examples so the model focuses more on hard, misclassified examples.\n- Incorporating a tunable focusing parameter (gamma) that controls the rate at which easy examples are down-weighted.\n- Supporting optional class weighting (alpha) to balance the importance of different classes.\n- Allowing label smoothing to reduce overconfidence and improve generalization.\n- Providing flexible reduction modes (mean, sum, or none) for the output loss.\n\nThis loss is particularly effective in tasks such as object detection or classification where there is a significant class imbalance.\n\n---\n","metadata":{}},{"cell_type":"code","source":"# --- 0. Squeeze-and-Excitation (SE) Block Implementation ---\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1) # Global average pooling across sequence dimension (for 1D feature vectors)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # x is assumed to be of shape [batch_size, feature_dim]\n        # For SE, we want to operate on channels. If x is 2D, we can treat the feature_dim as channels.\n        # But global average pooling expects [N, C, H, W] or similar.\n        # So, we'll simulate a (height, width) of 1 for 1D feature vectors.\n        b, c = x.size()\n        # Reshape to [B, C, 1] to apply AvgPool1d, then squeeze back to [B, C]\n        y = self.avg_pool(x.unsqueeze(-1)).squeeze(-1)\n        y = self.fc(y).view(b, c) # Reshape back to [B, C] for element-wise multiplication\n        return x * y.expand_as(x) # Element-wise scale\n\n# --- 0. Focal Loss Implementation ---\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean', epsilon=1e-12, label_smoothing=0.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.epsilon = epsilon\n        self.label_smoothing = label_smoothing\n\n        if self.alpha is not None:\n            if not isinstance(self.alpha, torch.Tensor):\n                self.alpha = torch.tensor(self.alpha, dtype=torch.float32)\n\n    def forward(self, inputs, targets):\n        # inputs: raw logits (before softmax/sigmoid) of shape (N, C)\n        # targets: class labels of shape (N,)\n\n        num_classes = inputs.shape[1]\n        \n        # Apply label smoothing to targets\n        if self.label_smoothing > 0:\n            smoothed_targets = torch.full_like(inputs, self.label_smoothing / (num_classes - 1))\n            # Scatter the 1.0-label_smoothing value to the true class position\n            smoothed_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n        else:\n            # Standard one-hot encoding if no smoothing\n            smoothed_targets = F.one_hot(targets, num_classes=num_classes).float()\n\n        # Compute log probabilities (log(pt))\n        log_pt = F.log_softmax(inputs, dim=1)\n        \n        # Calculate pt (probabilities) from log_pt for the focusing term\n        pt = torch.exp(log_pt)\n\n        # Get pt for the true (hard) class for the focusing term\n        # This uses the original (hard) target labels\n        pt_true_class = pt.gather(1, targets.long().unsqueeze(1)).squeeze()\n\n        # Compute the base loss, which is the smoothed cross-entropy\n        # Summing over the channels/classes dimension for the smoothed targets\n        base_loss = -(smoothed_targets * log_pt).sum(dim=1)\n        \n        # Focusing mechanism\n        focal_term = (1 - pt_true_class).pow(self.gamma)\n        loss = focal_term * base_loss\n\n        # Alpha weighting (applies to the loss per sample before reduction)\n        if self.alpha is not None:\n            if self.alpha.device != inputs.device:\n                self.alpha = self.alpha.to(inputs.device)\n            # alpha_t based on original hard target\n            alpha_t = self.alpha.gather(0, targets.long())\n            loss = alpha_t * loss\n        \n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else: # 'none'\n            return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Enhanced Supervised Contrastive Loss with Class Reweighting and Hard Negative Mining\n\nThis loss function extends the supervised contrastive learning objective with additional features to improve training robustness and handle class imbalance effectively.\n\n## Key Concepts:\n\n- **Contrastive Learning:** Encourages features of samples from the same class (positives) to be closer in the feature space, while pushing apart features from different classes (negatives).\n\n- **Multiple Views:** Can operate on multiple augmented views of each sample to create stronger representations. Supports two modes:\n  - `'all'`: Uses all views for contrastive pairs.\n  - `'one'`: Uses a single anchor and one contrastive view.\n\n- **Temperature Scaling:** Controls the sharpness of the similarity distribution used in the contrastive loss, helping with training stability.\n\n## Enhancements:\n\n1. **Class Reweighting:**\n   - Allows assigning different weights to each class.\n   - Helps mitigate class imbalance by giving more importance to under-represented classes.\n   - Weights are applied to the positive pairs' contribution to the loss.\n\n2. **Hard Negative Mining:**\n   - Instead of treating all negatives equally, it selects a fraction (`hard_mining_ratio`) of the hardest negatives (those most similar to the anchor).\n   - This focuses the training on the most confusing negative examples.\n   - A margin is used to push negatives away further, enhancing separation.\n\n3. **Dynamic Masking:**\n   - Constructs masks to identify positive pairs (same class) and exclude self-contrast pairs.\n   - Applies hard negative mining mask to include only selected negatives for the loss denominator.\n\n## Workflow Overview:\n\n- Features are normalized and cosine similarities between anchors and contrasts are computed.\n- Logits are adjusted for numerical stability.\n- Positive and negative pairs are masked.\n- Margin is applied to negatives to increase their separation.\n- Hard negatives are mined based on similarity scores.\n- Class weights are applied to positive pairs.\n- The final loss is computed as the weighted, temperature-scaled average of positive log probabilities.\n\n## Use Cases:\n\n- Suitable for imbalanced classification tasks where contrastive representation learning is beneficial.\n- Enhances robustness by focusing learning on challenging negative samples.\n- Useful in scenarios involving multi-view or augmented data representations.\n\n---\n","metadata":{}},{"cell_type":"code","source":"# --- Enhanced Supervised Contrastive Loss with Class Reweighting and Hard Negative Mining ---\nclass EnhancedSupConLoss(nn.Module):\n    def __init__(self, temperature=0.05, base_temperature=0.07, contrast_mode='all', \n                 hard_mining_ratio=0.35, margin=0.2):\n        super(EnhancedSupConLoss, self).__init__()\n        self.temperature = temperature\n        self.base_temperature = base_temperature\n        self.contrast_mode = contrast_mode\n        self.hard_mining_ratio = hard_mining_ratio  # Ratio of hard negatives to mine\n        self.margin = margin  # Margin to push negatives further\n        \n    def forward(self, features, labels=None, mask=None, class_weights=None):        \n        \"\"\"        \n        Args:        \n            features: hidden vector of shape [bsz, n_views, feature_dim] during training.\n                      During validation/inference, it might be [bsz, feature_dim]\n                      In that case, it's unsqueezed to [bsz, 1, feature_dim].\n            labels: ground truth of shape [bsz].        \n            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j        \n                  has the same class as sample i. Can be asymmetric.        \n            class_weights: dictionary mapping class indices to weights.        \n        Returns:        A loss scalar.        \n        \"\"\"\n        device = features.device\n\n        # Handle features shape: [bsz, n_views, feature_dim]\n        # If features are [bsz, feature_dim], unsqueeze to [bsz, 1, feature_dim] for consistency\n        if len(features.shape) < 3:\n            features = features.unsqueeze(1) # Add a view dimension if it's missing (e.g., in validation)\n        \n        batch_size = features.shape[0]\n        original_n_views = features.shape[1] # Number of views per original sample\n\n        if self.contrast_mode == 'one':\n            # For 'one' mode, we need at least two views (anchor, contrast)\n            if original_n_views < 2:\n                raise ValueError(\"`contrast_mode='one'` requires at least 2 views (e.g., [bsz, 2, feature_dim])\")\n            anchor_feature = features[:, 0]\n            contrast_feature = features[:, 1]\n            # Labels correspond to the original batch size for `contrast_mode='one'`\n            # and are not repeated for the features.\n        elif self.contrast_mode == 'all':\n            # Reshape to (batch_size * n_views, feature_dim)\n            anchor_feature = features.view(-1, features.shape[-1])\n            contrast_feature = anchor_feature\n            \n            if labels is not None:\n                # Repeat labels for each view to match the flattened feature size\n                labels = labels.repeat_interleave(original_n_views)\n                \n            batch_size = anchor_feature.shape[0] # Update batch_size after flattening\n        else:\n            raise ValueError('Unknown contrast mode: {}'.format(self.contrast_mode))\n            \n        # Compute logits (cosine similarity)\n        # Normalize features for cosine similarity\n        anchor_feature = F.normalize(anchor_feature, dim=1)\n        contrast_feature = F.normalize(contrast_feature, dim=1)\n\n        anchor_dot_contrast = torch.div(\n            torch.matmul(anchor_feature, contrast_feature.T),\n            self.temperature\n        )\n        \n        # For numerical stability (max trick) - apply before exp\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        anchor_dot_contrast = anchor_dot_contrast - logits_max.detach()\n        \n        # Mask diagonal (self-contrast) if contrast_feature is anchor_feature\n        # Only relevant for contrast_mode='all' where anchor_feature == contrast_feature\n        logits_mask = 1 - torch.eye(batch_size, device=device) if self.contrast_mode == 'all' else torch.ones(batch_size, batch_size, device=device)\n        \n        # Create mask for positive pairs\n        if mask is None: \n            mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float().to(device)\n        \n        # Hard negative mining: find the hardest negatives (highest similarity)\n        # Exclude positives and self-contrast from neg_mask\n        neg_mask = (1 - mask) * logits_mask # Mask for negative pairs, excluding self and positives\n        \n        # Apply margin to negative pairs (push them further away)\n        # Only modify the logits for actual negative pairs.\n        if self.margin > 0:\n            # Create a margined version of negative logits, only where neg_mask is active\n            margined_neg_logits = (anchor_dot_contrast * neg_mask) - (self.margin * neg_mask)\n            # Combine positives (original logits) with margined negatives\n            anchor_dot_contrast = (anchor_dot_contrast * mask) + margined_neg_logits\n        \n        # Hard negative mining: select the hardest negatives\n        # The goal here is to dynamically adjust `logits_mask` to only include the *k* hardest negatives\n        # for each anchor, in addition to all positive pairs.\n        if self.hard_mining_ratio < 1.0 and self.hard_mining_ratio > 0:\n            k = int(batch_size * self.hard_mining_ratio)\n            k = max(k, 1) # At least one negative if possible\n\n            # We need to compute the `exp_logits` and `log_prob` using the selected hard negatives.\n            # `logits_mask` will represent the final set of pairs to consider for the denominator of the loss.\n            # Start with a mask that includes all positives.\n            current_logits_mask = mask.clone() # This will be built upon\n\n            # Iterate through each anchor in the batch\n            for i in range(batch_size):\n                # Identify valid negative indices for the current anchor `i`\n                # Only consider where `neg_mask[i]` is 1 (actual negatives, not self or positives)\n                valid_neg_indices = torch.where(neg_mask[i] > 0)[0]\n\n                if len(valid_neg_indices) > 0:\n                    # Get the similarity scores for these valid negatives\n                    neg_sims = anchor_dot_contrast[i, valid_neg_indices]\n                \n                    # Select the top k hardest negatives (highest similarity)\n                    k_actual = min(k, len(valid_neg_indices))\n                    if k_actual > 0:\n                        # topk returns values and indices.\n                        # We only need indices.\n                        _, hard_neg_local_indices = torch.topk(neg_sims, k_actual)\n                        \n                        # Map local indices back to global batch indices\n                        hard_neg_global_indices = valid_neg_indices[hard_neg_local_indices]\n                    \n                        # Add these hard negatives to the current anchor's `logits_mask`\n                        current_logits_mask[i, hard_neg_global_indices] = 1.0\n                \n            logits_mask = current_logits_mask # Update the main logits_mask for calculations\n        \n        # The denominator for the log_prob will sum over all entries where logits_mask is 1\n        # This now includes positives AND selected hard negatives\n        exp_logits = torch.exp(anchor_dot_contrast) * logits_mask\n        log_prob = anchor_dot_contrast - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n        \n        # Calculate loss considering class weights for positive pairs\n        # The sum is only over positive pairs as defined by `mask`\n        positive_log_probs = log_prob * mask # This zeros out non-positive pairs for summing\n\n        # Sum of log probabilities for positive pairs\n        sum_positive_log_probs = positive_log_probs.sum(1)\n        # Count of positive pairs for normalization (add epsilon to avoid div by zero)\n        count_positive_pairs = mask.sum(1) + 1e-12\n\n        # Apply class weights to the mean of positive log probabilities\n        if class_weights is not None and labels is not None:\n            # Need to get original labels for weighting.\n            # If contrast_mode='all', labels were repeated, so take original batch labels.\n            if self.contrast_mode == 'all':\n                original_labels_for_weights = labels[::original_n_views] # Assumes labels were repeated for `original_n_views`\n            else: # contrast_mode='one'\n                original_labels_for_weights = labels\n\n            # Map class labels to their respective weights\n            weight_values = torch.tensor([class_weights.get(label.item(), 1.0)\n                                          for label in original_labels_for_weights], device=device)\n            \n            # Repeat weights for each view if contrast_mode='all' to match current labels dimension\n            if self.contrast_mode == 'all':\n                weight_values = weight_values.repeat_interleave(original_n_views)\n\n            # Apply weights to the mean log probability\n            # We are applying weight to each anchor's contribution\n            weighted_mean_log_prob_pos = (sum_positive_log_probs * weight_values) / count_positive_pairs\n        else:\n            weighted_mean_log_prob_pos = sum_positive_log_probs / count_positive_pairs\n        \n        # Final loss calculation\n        loss = -(self.temperature / self.base_temperature) * weighted_mean_log_prob_pos\n        loss = loss.mean() # Average across the batch\n        \n        return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Dataset for Multi-View Augmentation and Combined Images\n\n## Dataset Paths\n\n- `KAGGLE_INPUT_PATH`: Root directory containing the original dataset.\n- `PREPROCESSED_PATH`: Directory containing preprocessed face images.\n- Separate folders for training and validation data both in original and preprocessed forms:\n  - `TRAIN_PATH` and `VAL_PATH` for original images.\n  - `TRAIN_PREPROCESSED_PATH` and `VAL_PREPROCESSED_PATH` for preprocessed images.\n\n## GenderDataset Class Overview\n\nThis custom PyTorch `Dataset` is designed to handle paired inputs consisting of original and preprocessed face images along with their labels.\n\n### Key Features\n\n- **Data Loading:**\n  - Loads images from class-specific folders: `\"female\"` and `\"male\"`.\n  - Maintains consistent mapping: `female` â†’ 0, `male` â†’ 1.\n  - Loads both original and corresponding preprocessed images.\n  - Verifies existence of both original and preprocessed images to ensure pairing.\n  - Issues warnings for any missing directories or missing preprocessed images.\n\n- **Data Augmentation and Views:**\n  - Supports multi-view augmentations during training for contrastive learning.\n    - Applies two different random transformations to both original and preprocessed images, producing four augmented views per sample.\n  - For validation, applies a single deterministic transform to both images.\n  \n- **Handling Corrupted Images:**\n  - Includes exception handling when loading images.\n  - If loading fails, replaces images with a black placeholder image to prevent crashes.\n\n- **Label and Class Counts:**\n  - Stores labels alongside images.\n  - Keeps track of class counts for potential use in weighting or analysis.\n\n### Return Format\n\n- **Training mode:** Returns a tuple of four augmented views â€” two from original images, two from preprocessed images â€” along with the label.\n- **Validation mode:** Returns one transformed original image and one transformed preprocessed image with the label.\n\n---\n\nThis setup is useful for training models that leverage multiple views of both raw and preprocessed images, such as contrastive or multi-input networks for gender classification.\n","metadata":{}},{"cell_type":"code","source":"# Define paths\nKAGGLE_INPUT_PATH = '/kaggle/input/comsys-taska/Task_A'\nPREPROCESSED_PATH = '/kaggle/working/preprocessed_faces'\n\nTRAIN_PATH = os.path.join(KAGGLE_INPUT_PATH, 'train')\nVAL_PATH = os.path.join(KAGGLE_INPUT_PATH, 'val')\n\nTRAIN_PREPROCESSED_PATH = os.path.join(PREPROCESSED_PATH, 'train')\nVAL_PREPROCESSED_PATH = os.path.join(PREPROCESSED_PATH, 'val')\n\n# --- 1. Custom Dataset for Multi-View Augmentation and Combined Images ---\nclass GenderDataset(Dataset):\n    def __init__(self, data_dir, preprocessed_data_dir, transform=None, is_train=True):\n        self.data_dir = data_dir\n        self.preprocessed_data_dir = preprocessed_data_dir\n        self.transform = transform\n        self.is_train = is_train\n        self.image_paths = []\n        self.preprocessed_image_paths = []\n        self.labels = [] # 0 for female, 1 for male (consistent mapping)\n        self.class_to_idx = {'female': 0, 'male': 1}\n        self.idx_to_class = {0: 'female', 1: 'male'}\n        \n        print(f\"Loading dataset from: {data_dir} (is_train={is_train})\")\n\n        temp_image_paths = []\n        temp_preprocessed_image_paths = []\n        temp_labels = []\n\n        for gender in ['female', 'male']:\n            gender_path = os.path.join(data_dir, gender)\n            preprocessed_gender_path = os.path.join(preprocessed_data_dir, gender)\n            class_idx = self.class_to_idx[gender]\n            \n            if not os.path.exists(gender_path):\n                print(f\"Warning: Directory not found: {gender_path}. Skipping.\")\n                continue\n            if not os.path.exists(preprocessed_gender_path):\n                print(f\"Warning: Directory not found: {preprocessed_gender_path}. Skipping.\")\n                continue\n\n            for img_name in os.listdir(gender_path):\n                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n                    original_img_path = os.path.join(gender_path, img_name)\n                    # Construct preprocessed image path\n                    preprocessed_img_name = f\"preprocessed_face_{img_name}\" # Assuming this naming convention from preprocessing\n                    preprocessed_img_path = os.path.join(preprocessed_gender_path, preprocessed_img_name)\n\n                    if os.path.exists(preprocessed_img_path):\n                        temp_image_paths.append(original_img_path)\n                        temp_preprocessed_image_paths.append(preprocessed_img_path)\n                        temp_labels.append(class_idx)\n                    else:\n                        print(f\"Warning: Corresponding preprocessed image not found for {original_img_path}. Skipping.\")\n        \n        # Apply outlier detection only for training data if threshold is provided\n        self.image_paths = temp_image_paths\n        self.preprocessed_image_paths = temp_preprocessed_image_paths\n        self.labels = temp_labels\n\n        self.class_counts = Counter(self.labels)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        preprocessed_img_path = self.preprocessed_image_paths[idx]\n        label = self.labels[idx]\n        \n        try:\n            img = Image.open(img_path).convert('RGB') # Load as RGB\n        except Exception as e:\n            print(f\"Error loading original image {img_path}: {e}\")\n            img = Image.new('RGB', (224, 224), color='black')\n\n        try:\n            preprocessed_img = Image.open(preprocessed_img_path).convert('RGB') # Load as RGB\n        except Exception as e:\n            print(f\"Error loading preprocessed image {preprocessed_img_path}: {e}\")\n            preprocessed_img = Image.new('RGB', (224, 224), color='black')\n\n        if self.transform:\n            if self.is_train:\n                # Apply two different random augmentations for contrastive learning\n                img1_original = self.transform(img)\n                img2_original = self.transform(img)\n\n                img1_processed = self.transform(preprocessed_img)\n                img2_processed = self.transform(preprocessed_img)\n                \n                # Return tuples of (original_view1, processed_view1), (original_view2, processed_view2)\n                return (img1_original, img2_original, img1_processed, img2_processed), label\n            else:\n                # For validation, apply a single, deterministic transform\n                img_transformed = self.transform(img)\n                preprocessed_img_transformed = self.transform(preprocessed_img)\n                return (img_transformed, preprocessed_img_transformed), label\n        \n        return (img, preprocessed_img), label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PyTorch Lightning DataModule for Gender Classification\n\n## Purpose\n\nThis `GenderDataModule` class is designed to streamline data loading, preprocessing, and batching for training and validation in a PyTorch Lightning workflow. It handles paired datasets of original and preprocessed face images with support for class imbalance correction.\n\n---\n\n## Key Components\n\n### Initialization\n\n- Takes directories for:\n  - Training original images\n  - Validation original images\n  - Training preprocessed images\n  - Validation preprocessed images\n- Configurable batch size, number of workers, and image size.\n- Defines data augmentation and normalization pipelines for training and validation:\n  - **Training augmentations** include resizing, cropping, flips, color jitter, rotations, blurring, perspective distortions, and random erasing â€” enhancing robustness.\n  - **Validation transforms** apply standard resizing and center cropping, with normalization.\n\n### Setup Method\n\n- Loads training and validation datasets using the custom `GenderDataset` class.\n- Calculates class counts and computes **class weights** using inverse frequency:\n  - This weighting mitigates class imbalance by giving higher loss weight to minority classes.\n- Stores these weights both as a dictionary and a tensor for later use.\n\n### Data Loaders\n\n- **Training DataLoader:**\n  - Uses a `WeightedRandomSampler` to balance sampling according to class weights.\n  - Ensures the model sees a balanced representation of classes each epoch despite dataset imbalance.\n- **Validation DataLoader:**\n  - Provides deterministic batches without shuffling.\n\n---\n\n## Benefits\n\n- Automated multi-view augmentation suited for contrastive or multi-input models.\n- Handles class imbalance effectively during sampling and loss computation.\n- Clean integration with PyTorch Lightning for easy training loops.\n\n---\n\nThis DataModule is a solid base for training robust gender classification models using both raw and preprocessed face images.\n","metadata":{}},{"cell_type":"code","source":"# --- 2. PyTorch Lightning DataModule ---\nclass GenderDataModule(pl.LightningDataModule):\n    def __init__(self, train_data_dir, val_data_dir, train_preprocessed_data_dir, val_preprocessed_data_dir, batch_size=64, num_workers=4, image_size=(224, 224)):\n        super().__init__()\n        self.train_data_dir = train_data_dir\n        self.val_data_dir = val_data_dir\n        self.train_preprocessed_data_dir = train_preprocessed_data_dir\n        self.val_preprocessed_data_dir = val_preprocessed_data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.image_size = image_size\n\n        # ImageNet mean and std for normalization as recommended by models\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n        # Define transformations\n        self.train_transform = transforms.Compose([\n            transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0), ratio=(0.75, 1.33)), # Simulates scale and crop\n            transforms.RandomHorizontalFlip(), # Geometric transformation\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1), # Color space transformation\n            transforms.RandomRotation(degrees=20), # Geometric transformation\n            transforms.GaussianBlur(kernel_size=3), # Add some blur/noise\n            transforms.RandomPerspective(distortion_scale=0.2, p=0.5), # New: Perspective distortion\n            transforms.ToTensor(), # Convert PIL Image to PyTorch Tensor\n            self.normalize, # Normalize pixel values\n            transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3)) # Simulates occlusions\n        ])\n        \n        self.val_transform = transforms.Compose([\n            transforms.Resize(int(image_size[0] / 0.875)), # Standard resize\n            transforms.CenterCrop(image_size), # Standard center crop\n            transforms.ToTensor(),\n            self.normalize\n        ])\n\n        self.train_dataset = None\n        self.val_dataset = None\n        self.class_weights_for_loss = None\n        self.class_weights_tensor = None\n\n    def setup(self, stage=None):\n        if stage == 'fit' or stage is None:\n            self.train_dataset = GenderDataset(self.train_data_dir, self.train_preprocessed_data_dir, transform=self.train_transform, is_train=True)\n            self.val_dataset = GenderDataset(self.val_data_dir, self.val_preprocessed_data_dir, \n                                             transform=self.val_transform, is_train=False)\n\n            total_samples = len(self.train_dataset)\n            num_classes = len(self.train_dataset.class_to_idx)\n            \n            female_count = self.train_dataset.class_counts.get(0, 0)\n            male_count = self.train_dataset.class_counts.get(1, 0)\n\n            # Inverse frequency weighting: total_samples / (num_classes * class_count)\n            weight_female = total_samples / (num_classes * female_count) if female_count > 0 else 1.0\n            weight_male = total_samples / (num_classes * male_count) if male_count > 0 else 1.0\n            \n            self.class_weights_for_loss = {0: weight_female, 1: weight_male}\n            self.class_weights_tensor = torch.tensor([weight_female, weight_male], dtype=torch.float32)\n\n            print(f\"Calculated class weights for loss (Female: {self.class_weights_for_loss[0]:.2f}, Male: {self.class_weights_for_loss[1]:.2f})\")\n            print(f\"Train dataset class counts: {self.train_dataset.class_counts}\")\n\n\n    def train_dataloader(self):\n        labels = self.train_dataset.labels\n        sample_weights = [self.class_weights_for_loss[label] for label in labels]\n        \n        sample_weights = np.array(sample_weights)\n        sample_weights[~np.isfinite(sample_weights)] = 1.0\n\n        sampler = WeightedRandomSampler(\n            weights=list(sample_weights),\n            num_samples=len(sample_weights),\n            replacement=True\n        )\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GenderClassificationModel Overview\n\n## Model Architecture\n\n- **Dual EfficientNet-B3 Backbones**  \n  Extract feature embeddings independently from:\n  - Original images  \n  - Preprocessed images  \n  These embeddings are concatenated to form a richer combined representation.\n\n- **Squeeze-and-Excitation (SE) Block**  \n  Applies channel-wise attention to recalibrate the concatenated features, improving representational power.\n\n- **Projection Head**  \n  Maps SE-recalibrated features to a lower-dimensional embedding space (128 dims) used for **Supervised Contrastive Loss** (SupCon).\n\n- **Classification Head**  \n  A strengthened multi-layer perceptron with dropout, producing logits for the two-class gender classification task.\n\n- **Learnable Loss Weight Parameter**  \n  Dynamically balances the importance between the SupCon loss and the classification loss during training.\n\n---\n\n## Loss Functions\n\n- **Enhanced Supervised Contrastive Loss (SupCon)**  \n  Encourages the model to learn discriminative embeddings by pulling together samples of the same class and pushing apart samples of different classes.\n\n- **Focal Loss with Label Smoothing**  \n  Combats class imbalance by focusing training on hard-to-classify samples and softening hard labels to improve generalization.\n\n- Class-wise weighting can be incorporated in focal loss based on dataset imbalance statistics.\n\n---\n\n## Forward Pass Modes\n\n- **Training (4-tensor tuple input)**  \n  Receives pairs of original and preprocessed images with two augmented views each.  \n  Extracts features, recalibrates, projects for contrastive loss, and outputs classification logits.\n\n- **Validation/Inference (2-tensor tuple input)**  \n  Processes single views of original and preprocessed images to output classification logits.\n\n---\n\n## Training & Validation Steps\n\n- Calculates both SupCon loss and focal classification loss, combined dynamically with the learnable weight parameter.  \n- Logs losses and metrics (accuracy, F1, ROC-AUC, PR-AUC) at both batch and epoch levels.  \n- Accumulates raw predictions and labels across batches for comprehensive epoch-end metrics.\n\n---\n\n## Optimizer & Scheduler\n\n- AdamW optimizer with differentiated learning rates:\n  - Backbone models get a base LR.\n  - Projection and classification heads get a higher LR.\n  - The learnable loss weighting parameter has the highest LR for faster adaptation.\n\n- **Cosine Annealing Warm Restarts** scheduler to cyclically adjust learning rates, enhancing convergence.\n\n---\n\n## Utility: Visualization of Misclassifications\n\n- Visualizes misclassified validation images by displaying original images alongside their true and predicted labels.  \n- Supports showing a user-specified number of misclassifications or all if unspecified.\n\n---\n\n# Summary\n\nThis model elegantly fuses state-of-the-art CNN feature extractors with contrastive and classification objectives, balanced dynamically to leverage complementary training signals. It addresses data imbalance with focal loss and class weighting, and includes mechanisms for robust training monitoring and error analysis.\n","metadata":{}},{"cell_type":"code","source":"# --- 3. PyTorch Lightning Model with EfficientNetB3 and EnhancedSupConLoss ---\nclass GenderClassificationModel(pl.LightningModule):\n    def __init__(self, num_classes=2, learning_rate=1e-4, weight_decay=1e-5, \n                 supcon_temp=0.07, supcon_base_temp=0.07, supcon_hard_mining_ratio=0.35, \n                 supcon_margin=0.2, class_weights_for_loss=None, class_weights_tensor=None,\n                 max_epochs: int = 50, label_smoothing: float = 0.0, gamma: float = 2.0): # Added gamma here\n        super().__init__()\n        self.save_hyperparameters() # Saves all init args as self.hparams\n\n        # Feature extractor for original images - using EfficientNetB3\n        # num_classes=0 means it returns features without a classification head\n        self.feature_extractor_original = timm.create_model('efficientnet_b3', pretrained=True, num_classes=0)\n        # Feature dimension for efficientnet_b3 is 1536\n        efficientnet_feature_dim = self.feature_extractor_original.num_features \n\n        # Feature extractor for preprocessed images - using EfficientNetB3\n        self.feature_extractor_processed = timm.create_model('efficientnet_b3', pretrained=True, num_classes=0)\n        \n        # Combined feature dimension before projection/classification heads\n        combined_feature_dim = efficientnet_feature_dim * 2\n\n        # Squeeze-and-Excitation (SE) block for channel-wise feature recalibration\n        self.se_block = SEBlock(channel=combined_feature_dim)\n        \n        # Learnable parameter for weighting between contrastive and classification loss\n        self.loss_weight_param = nn.Parameter(torch.tensor(0.5, dtype=torch.float32)) # Initial weight of 0.5\n\n        # Projection head for Supervised Contrastive Loss (takes SE-recalibrated concatenated features)\n        self.projection_head = nn.Sequential(\n            nn.Linear(combined_feature_dim, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 128) # Output embedding dimension for contrastive loss\n        )\n        \n        # Strengthened Classification Head (takes SE-recalibrated concatenated features)\n        self.classification_head = nn.Sequential(\n            nn.Linear(combined_feature_dim, 256), # First hidden layer\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3), # Added dropout for regularization\n            nn.Linear(256, num_classes) # Output layer\n        )\n\n        self.supcon_loss_fn = EnhancedSupConLoss(\n            temperature=supcon_temp,\n            base_temperature=supcon_base_temp,\n            contrast_mode='all',\n            hard_mining_ratio=supcon_hard_mining_ratio, \n            margin=supcon_margin \n        )\n        \n        if class_weights_tensor is not None:\n            alpha_for_focal = torch.tensor([class_weights_for_loss[0], class_weights_for_loss[1]], dtype=torch.float32)\n        else:\n            alpha_for_focal = None\n\n        self.focal_loss_fn = FocalLoss(\n            alpha=alpha_for_focal, # Pass class-wise alpha\n            gamma=self.hparams.gamma, # Gamma now correctly accessed from hparams\n            label_smoothing=label_smoothing # Pass label smoothing parameter\n        )\n        \n        self.class_weights_for_loss = class_weights_for_loss\n        self.class_weights_tensor = class_weights_tensor\n\n        self.train_raw_preds = []\n        self.train_labels = []\n        self.val_raw_preds = []\n        self.val_labels = []\n\n    def forward(self, x):\n        if isinstance(x, tuple) and len(x) == 4: # Training phase: (img1_original, img2_original, img1_processed, img2_processed)\n            img1_original, img2_original, img1_processed, img2_processed = x\n\n            # Extract features from both original and preprocessed images\n            features1_original = self.feature_extractor_original(img1_original)\n            features2_original = self.feature_extractor_original(img2_original)\n            features1_processed = self.feature_extractor_processed(img1_processed)\n            features2_processed = self.feature_extractor_processed(img2_processed)\n\n            # Concatenate features from corresponding original and processed views\n            features1_combined = torch.cat((features1_original, features1_processed), dim=1)\n            features2_combined = torch.cat((features2_original, features2_processed), dim=1)\n            \n            # Apply SE Block for feature recalibration after concatenation\n            features1_combined_se = self.se_block(features1_combined)\n            features2_combined_se = self.se_block(features2_combined)\n\n            # Apply projection head for contrastive loss\n            proj_features1 = self.projection_head(features1_combined_se)\n            proj_features2 = self.projection_head(features2_combined_se)\n            \n            supcon_features = torch.stack((proj_features1, proj_features2), dim=1)\n            \n            # Use the combined features from the first view for classification\n            logits = self.classification_head(features1_combined_se) \n            return supcon_features, logits\n        elif isinstance(x, tuple) and len(x) == 2: # Validation/inference phase: (img_original, img_processed)\n            img_original, img_processed = x\n            features_original = self.feature_extractor_original(img_original)\n            features_processed = self.feature_extractor_processed(img_processed)\n            \n            # Concatenate features for inference\n            features_combined = torch.cat((features_original, features_processed), dim=1)\n            \n            # Apply SE Block for feature recalibration\n            features_combined_se = self.se_block(features_combined)\n            \n            logits = self.classification_head(features_combined_se)\n            return logits\n        else:\n            raise ValueError(\"Unexpected input format for forward pass. Expected tuple of 2 or 4 tensors.\")\n\n\n    def training_step(self, batch, batch_idx):\n        (imgs1_original, imgs2_original, imgs1_processed, imgs2_processed), labels = batch\n        \n        supcon_features, logits = self((imgs1_original, imgs2_original, imgs1_processed, imgs2_processed))\n\n        supcon_loss = self.supcon_loss_fn(\n            features=supcon_features,\n            labels=labels,\n            class_weights=self.class_weights_for_loss\n        )\n\n        ce_loss = self.focal_loss_fn(logits, labels)\n        \n        # Dynamically weight the losses\n        total_loss = self.loss_weight_param * supcon_loss + (1 - self.loss_weight_param) * ce_loss\n        \n        self.log('train_supcon_loss', supcon_loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_ce_loss', ce_loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('loss_weight_param', self.loss_weight_param, on_step=True, on_epoch=True, prog_bar=True)\n\n\n        self.train_raw_preds.append(F.softmax(logits, dim=1).detach().cpu().numpy())\n        self.train_labels.append(labels.cpu().numpy())\n        \n        return total_loss\n\n    def validation_step(self, batch, batch_idx):\n        (imgs_original, imgs_processed), labels = batch\n        logits = self((imgs_original, imgs_processed))\n        \n        loss = self.focal_loss_fn(logits, labels)\n        \n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.val_raw_preds.append(F.softmax(logits, dim=1).detach().cpu().numpy())\n        self.val_labels.append(labels.cpu().numpy())\n        \n        return loss\n\n    def on_train_epoch_end(self):\n        if len(self.train_raw_preds) == 0:\n            return\n        \n        all_raw_preds = np.concatenate(self.train_raw_preds)\n        all_labels = np.concatenate(self.train_labels)\n        \n        all_preds_classes = np.argmax(all_raw_preds, axis=1)\n\n        accuracy = accuracy_score(all_labels, all_preds_classes)\n        self.log('train_accuracy_epoch', accuracy, prog_bar=True)\n\n        # Calculate F1-score\n        f1 = f1_score(all_labels, all_preds_classes, average='weighted', zero_division=0)\n        self.log('train_f1_epoch', f1, prog_bar=True)\n\n        # Calculate ROC-AUC\n        try:\n            if len(np.unique(all_labels)) > 1:\n                roc_auc = roc_auc_score(all_labels, all_raw_preds[:, 1], average='weighted')\n                self.log('train_roc_auc_epoch', roc_auc, prog_bar=True)\n            else:\n                self.log('train_roc_auc_epoch', 0.0, prog_bar=True)\n        except ValueError:\n            self.log('train_roc_auc_epoch', 0.0, prog_bar=True)\n\n        # Calculate PR-AUC\n        try:\n            if len(np.unique(all_labels)) > 1:\n                precision, recall, _ = precision_recall_curve(all_labels, all_raw_preds[:, 1])\n                pr_auc = auc(recall, precision)\n                self.log('train_pr_auc_epoch', pr_auc, prog_bar=True)\n            else:\n                self.log('train_pr_auc_epoch', 0.0, prog_bar=True)\n        except ValueError:\n            self.log('train_pr_auc_epoch', 0.0, prog_bar=True)\n        \n        self.train_raw_preds.clear()\n        self.train_labels.clear()\n\n    def on_validation_epoch_end(self):\n        if len(self.val_raw_preds) == 0:\n            return\n        \n        all_raw_preds = np.concatenate(self.val_raw_preds)\n        all_labels = np.concatenate(self.val_labels)\n        \n        all_preds_classes = np.argmax(all_raw_preds, axis=1)\n\n        accuracy = accuracy_score(all_labels, all_preds_classes)\n        self.log('val_accuracy_epoch', accuracy, prog_bar=True)\n\n        f1 = f1_score(all_labels, all_preds_classes, average='weighted', zero_division=0)\n        self.log('val_f1_epoch', f1, prog_bar=True)\n\n        try:\n            if len(np.unique(all_labels)) > 1:\n                roc_auc = roc_auc_score(all_labels, all_raw_preds[:, 1], average='weighted')\n                self.log('val_roc_auc_epoch', roc_auc, prog_bar=True)\n            else:\n                self.log('val_roc_auc_epoch', 0.0, prog_bar=True)\n        except ValueError:\n            self.log('val_roc_auc_epoch', 0.0, prog_bar=True)\n\n        try:\n            if len(np.unique(all_labels)) > 1:\n                precision, recall, _ = precision_recall_curve(all_labels, all_raw_preds[:, 1])\n                pr_auc = auc(recall, precision)\n                self.log('val_pr_auc_epoch', pr_auc, prog_bar=True)\n            else:\n                self.log('val_pr_auc_epoch', 0.0, prog_bar=True)\n        except ValueError:\n            self.log('val_pr_auc_epoch', 0.0, prog_bar=True)\n        \n        self.val_raw_preds.clear()\n        self.val_labels.clear()\n\n    def configure_optimizers(self):\n        # Separate optimizer for backbone vs. heads (including loss_weight_param)\n        # This allows different learning rates for different parts of the model\n        optimizer_params = [\n            {'params': self.feature_extractor_original.parameters(), 'lr': self.hparams.learning_rate},\n            {'params': self.feature_extractor_processed.parameters(), 'lr': self.hparams.learning_rate},\n            {'params': self.se_block.parameters(), 'lr': self.hparams.learning_rate},\n            {'params': self.projection_head.parameters(), 'lr': self.hparams.learning_rate * 2}, # Higher LR for heads\n            {'params': self.classification_head.parameters(), 'lr': self.hparams.learning_rate * 2}, # Higher LR for heads\n            {'params': self.loss_weight_param, 'lr': self.hparams.learning_rate * 5} # Even higher LR for the loss weight\n        ]\n\n        optimizer = torch.optim.AdamW(optimizer_params, weight_decay=self.hparams.weight_decay)\n        \n        # Using Cosine Annealing Warm Restarts scheduler\n        scheduler = {\n            'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                optimizer,\n                T_0=self.hparams.max_epochs // 5, # Number of epochs for the first restart cycle\n                T_mult=2, # Multiplier for T_0 after each restart (cycle length increases)\n                eta_min=self.hparams.learning_rate / 100, # Minimum learning rate\n                verbose=True\n            ),\n            'monitor': 'val_loss', # Still monitor val_loss for logging/debugging purposes, though LR changes by itself\n            'interval': 'epoch',\n            'frequency': 1\n        }\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n# --- Utility Function for Final Evaluation on Train and Val Sets ---\ndef visualize_misclassifications(model, dataloader, class_names, num_images=None): # Changed default to None\n    \"\"\"\n    Visualizes misclassified images with their true and predicted labels.\n    If num_images is None, all misclassified images are shown.\n    \"\"\"\n    misclassified_samples = []\n    \n    # Ensure model is in evaluation mode\n    model.eval()\n    device = next(model.parameters()).device # Get current device of the model\n\n    with torch.no_grad():\n        for batch_idx, (imgs_tuple, labels) in enumerate(dataloader):\n            # Unpack the tuple of original and preprocessed images\n            imgs_original, imgs_processed = imgs_tuple \n            \n            imgs_original = imgs_original.to(device)\n            imgs_processed = imgs_processed.to(device)\n            labels = labels.to(device)\n\n            logits = model((imgs_original, imgs_processed)) # Pass as tuple\n            predicted_probs = F.softmax(logits, dim=1)\n            predicted_labels = torch.argmax(predicted_probs, dim=1)\n\n            for i in range(len(labels)):\n                if predicted_labels[i] != labels[i]:\n                    # Detach the ORIGINAL image and move to CPU for plotting\n                    # We plot original to see the actual input, not the preprocessed version\n                    img_cpu = imgs_original[i].cpu() \n                    # Denormalize image for display (reverse ImageNet normalization)\n                    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n                    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n                    img_display = img_cpu * std + mean\n                    img_display = torch.clamp(img_display, 0, 1) # Clamp to [0,1]\n                    \n                    misclassified_samples.append({\n                        'image': img_display,\n                        'true_label': class_names[labels[i].item()],\n                        'predicted_label': class_names[predicted_labels[i].item()]\n                    })\n            # If num_images is specified, stop collecting after that many\n            if num_images is not None and len(misclassified_samples) >= num_images:\n                break\n    \n    if misclassified_samples:\n        display_count = len(misclassified_samples) if num_images is None else min(num_images, len(misclassified_samples))\n        print(f\"\\n--- Visualizing {display_count} Misclassified Images ---\")\n        \n        # Calculate grid dimensions: aiming for roughly square layout\n        cols = 5\n        rows = (display_count + cols - 1) // cols\n        \n        plt.figure(figsize=(15, 4 * rows)) # Adjust figure size dynamically\n        for i, sample in enumerate(misclassified_samples[:display_count]):\n            plt.subplot(rows, cols, i + 1)\n            plt.imshow(sample['image'].permute(1, 2, 0).numpy()) # Convert C,H,W to H,W,C for imshow\n            plt.title(f\"True: {sample['true_label']}\\nPred: {sample['predicted_label']}\")\n            plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"\\nNo misclassified images found in the validation set (or accuracy is very high)!\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Detailed Model Evaluation Report Generator\n\nThis document explains the function `generate_detailed_reports` which loads a trained classification model, performs inference on both training and validation datasets, and produces comprehensive evaluation reports including accuracy, classification reports, confusion matrices, and ROC/PR AUC scores.\n\n---\n\n## Function Overview\n\n`generate_detailed_reports(model_path, data_module_instance, class_names)`\n\n- **Purpose:**  \n  Load the best checkpoint of a gender classification model, run inference on training and validation sets, and generate detailed performance metrics and visualizations.\n\n- **Inputs:**  \n  - `model_path` : Path to the saved model checkpoint (.ckpt) file.  \n  - `data_module_instance` : An instance of a data module providing data loaders and transforms.  \n  - `class_names` : List of class names corresponding to the classification labels.\n\n- **Outputs:**  \n  Prints detailed classification metrics and shows confusion matrix heatmaps for both datasets.\n\n---\n\n## Detailed Steps\n\n### 1. Re-initialize Data Module  \n- To obtain class weights and other dataset parameters, the data module is re-instantiated with the same parameters and setup for 'fit'.\n\n### 2. Load the Best Model Checkpoint  \n- The model checkpoint is loaded with necessary hyperparameters such as learning rate, weight decay, temperature parameters for supervised contrastive loss, class weights, etc.  \n- The model is set to evaluation mode and frozen to disable training behavior.\n\n### 3. Inference on Training Set  \n- A DataLoader for the training set is created without augmentation and shuffling, ensuring a consistent evaluation.  \n- Predictions (softmax probabilities) and true labels are collected for the entire training set.\n\n### 4. Training Set Evaluation  \n- Overall accuracy is computed.  \n- A detailed classification report is printed including precision, recall, and F1-score for each class.  \n- Confusion matrix is generated and visualized as a heatmap.  \n- ROC-AUC and Precision-Recall AUC scores are calculated if multiple classes are present.\n\n### 5. Inference on Validation Set  \n- Similar steps are followed as for the training set but using the validation dataloader provided by the data module.  \n- Predictions and labels are collected for the validation set.\n\n### 6. Validation Set Evaluation  \n- Accuracy, classification report, confusion matrix visualization, ROC-AUC, and PR-AUC scores are computed and displayed similarly.\n\n---","metadata":{}},{"cell_type":"code","source":"def generate_detailed_reports(model_path, data_module_instance, class_names):\n    \"\"\"\n    Loads the best model, runs inference on both train and validation sets,\n    and generates comprehensive classification reports and confusion matrices for both.\n    Also visualizes misclassifications.\n    \"\"\"\n    print(f\"\\n--- Generating detailed reports using model: {model_path} ---\")\n\n    # Re-instantiate DataModule to get its class weights, which are needed by the model's init.\n    temp_data_module = GenderDataModule(\n        train_data_dir=data_module_instance.train_data_dir,\n        val_data_dir=data_module_instance.val_data_dir,\n        train_preprocessed_data_dir=data_module_instance.train_preprocessed_data_dir,\n        val_preprocessed_data_dir=data_module_instance.val_preprocessed_data_dir,\n        batch_size=data_module_instance.batch_size,\n        num_workers=data_module_instance.num_workers,\n    )\n    temp_data_module.setup('fit')\n\n    best_model = GenderClassificationModel.load_from_checkpoint(\n        model_path,\n        map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n        num_classes=2,\n        learning_rate=LEARNING_RATE, # This LR will be overridden by scheduler's behavior at inference\n        weight_decay=WEIGHT_DECAY, \n        supcon_temp=SUPCON_TEMP, \n        supcon_base_temp=SUPCON_BASE_TEMP, \n        supcon_hard_mining_ratio=SUPCON_HARD_MINING_RATIO, \n        supcon_margin=SUPCON_MARGIN, \n        class_weights_for_loss=temp_data_module.class_weights_for_loss,\n        class_weights_tensor=temp_data_module.class_weights_tensor,\n        max_epochs=MAX_EPOCHS, # Pass max_epochs when loading\n        label_smoothing=LABEL_SMOOTHING_EPSILON, # Pass label_smoothing when loading\n        gamma=FOCAL_LOSS_GAMMA # Pass gamma when loading\n    )\n    best_model.eval() # Set model to evaluation mode\n    best_model.freeze() # Freeze layers for inference\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    best_model.to(device) # Move model to appropriate device\n\n    # --- Evaluate on Training Set ---\n    # Need a DataLoader for the training set without WeightedRandomSampler for evaluation.\n    # IMPORTANT: For evaluation, use is_train=False to get single view per image\n    train_eval_dataset = GenderDataset(data_module_instance.train_data_dir, \n                                       data_module_instance.train_preprocessed_data_dir,\n                                       transform=data_module_instance.val_transform,\n                                       is_train=False) # Use False for consistent evaluation (no double augmentation)\n    train_eval_dataloader = DataLoader(\n        train_eval_dataset,\n        batch_size=data_module_instance.batch_size,\n        shuffle=False,\n        num_workers=data_module_instance.num_workers,\n        pin_memory=True\n    )\n    \n    all_train_labels = []\n    all_train_preds_probs = []\n\n    print(\"\\nCollecting training set predictions for report...\")\n    for batch in tqdm(train_eval_dataloader, desc=\"Training Set Inference\"):\n        imgs_original, imgs_processed = batch[0] # Unpack the tuple for original and processed\n        labels = batch[1]\n        \n        imgs_original = imgs_original.to(device)\n        imgs_processed = imgs_processed.to(device)\n        \n        with torch.no_grad():\n            logits = best_model((imgs_original, imgs_processed)) # Pass as tuple\n            probs = F.softmax(logits, dim=1)\n        \n        all_train_labels.extend(labels.cpu().numpy())\n        all_train_preds_probs.extend(probs.cpu().numpy())\n\n    all_train_labels = np.array(all_train_labels)\n    all_train_preds_probs = np.array(all_train_preds_probs)\n    all_train_preds_classes = np.argmax(all_train_preds_probs, axis=1)\n\n    print(\"\\n--- Comprehensive Evaluation Report (Training Set) ---\")\n    accuracy_train = accuracy_score(all_train_labels, all_train_preds_classes)\n    print(f\"Overall Accuracy (Training Set): {accuracy_train:.4f}\")\n    print(\"\\nDetailed Classification Report (Training Set):\")\n    print(classification_report(all_train_labels, all_train_preds_classes, target_names=class_names, zero_division=0))\n\n    print(\"\\n--- Confusion Matrix (Training Set) ---\")\n    cm_train = confusion_matrix(all_train_labels, all_train_preds_classes)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix (Training Set)')\n    plt.show()\n\n    try:\n        if len(np.unique(all_train_labels)) > 1:\n            roc_auc_train = roc_auc_score(all_train_labels, all_train_preds_probs[:, 1], average='weighted')\n            print(f\"Final Training ROC-AUC (weighted): {roc_auc_train:.4f}\")\n        else:\n            print(\"Cannot compute ROC-AUC: Only one class present in training labels.\")\n    except Exception as e:\n        print(f\"Error computing final Training ROC-AUC: {e}\")\n\n    try:\n        if len(np.unique(all_train_labels)) > 1:\n            precision_train, recall_train, _ = precision_recall_curve(all_train_labels, all_train_preds_probs[:, 1])\n            pr_auc_train = auc(recall_train, precision_train)\n            print(f\"Final Training PR-AUC (weighted): {pr_auc_train:.4f}\")\n        else:\n            print(\"Cannot compute PR-AUC: Only one class present in training labels.\")\n    except Exception as e:\n        print(f\"Error computing final Training PR-AUC: {e}\")\n\n\n    # --- Evaluate on Validation Set ---\n    val_dataloader = data_module_instance.val_dataloader()\n    \n    all_val_labels = []\n    all_val_preds_probs = []\n\n    print(\"\\nCollecting validation set predictions for report...\")\n    for batch in tqdm(val_dataloader, desc=\"Validation Set Inference\"):\n        imgs_original, imgs_processed = batch[0] # Unpack the tuple for original and processed\n        labels = batch[1]\n\n        imgs_original = imgs_original.to(device)\n        imgs_processed = imgs_processed.to(device)\n      \n        with torch.no_grad():\n            logits = best_model((imgs_original, imgs_processed)) # Pass as tuple\n            probs = F.softmax(logits, dim=1)\n        \n        all_val_labels.extend(labels.cpu().numpy())\n        all_val_preds_probs.extend(probs.cpu().numpy())\n\n    all_val_labels = np.array(all_val_labels)\n    all_val_preds_probs = np.array(all_val_preds_probs)\n    all_val_preds_classes = np.argmax(all_val_preds_probs, axis=1)\n\n    print(\"\\n--- Comprehensive Evaluation Report (Validation Set) ---\")\n    accuracy_val = accuracy_score(all_val_labels, all_val_preds_classes)\n    print(f\"Overall Accuracy (Validation Set): {accuracy_val:.4f}\")\n    print(\"\\nDetailed Classification Report (Validation Set):\")\n    print(classification_report(all_val_labels, all_val_preds_classes, target_names=class_names, zero_division=0))\n\n    print(\"\\n--- Confusion Matrix (Validation Set) ---\")\n    cm_val = confusion_matrix(all_val_labels, all_val_preds_classes)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix (Validation Set)')\n    plt.show()\n\n    try:\n        if len(np.unique(all_val_labels)) > 1:\n            roc_auc_val = roc_auc_score(all_val_labels, all_val_preds_probs[:, 1], average='weighted')\n            print(f\"Final Validation ROC-AUC (weighted): {roc_auc_val:.4f}\")\n        else:\n            print(\"Cannot compute ROC-AUC: Only one class present in validation labels.\")\n    except Exception as e:\n        print(f\"Error computing final Validation ROC-AUC: {e}\")\n\n    try:\n        if len(np.unique(all_val_labels)) > 1:\n            precision_val, recall_val, _ = precision_recall_curve(all_val_labels, all_val_preds_probs[:, 1])\n            pr_auc_val = auc(recall_val, precision_val)\n            print(f\"Final Validation PR-AUC (weighted): {pr_auc_val:.4f}\")\n        else:\n            print(\"Cannot compute PR-AUC: Only one class present in validation labels.\")\n    except Exception as e:\n        print(f\"Error computing final Validation PR-auc: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Training Script Overview\n\n## Hyperparameters\n\n- **Batch Size:** 16 (recommended low due to two EfficientNet models)  \n- **Number of Workers:** 4 (increase if CPU resources allow)  \n- **Learning Rate:** 5e-5 (fine-tuned for EfficientNet)  \n- **Weight Decay:** 1e-5 (L2 regularization)  \n- **Max Epochs:** 50 (enables scheduler effectiveness)  \n- **Label Smoothing Epsilon:** 0.1 (helps generalization by smoothing labels)  \n- **Focal Loss Gamma:** 3.0 (focuses training on hard examples)\n\n## Supervised Contrastive Loss Parameters\n\n- **Temperature:** 0.07  \n- **Base Temperature:** 0.07  \n- **Hard Mining Ratio:** 0.5 (aggressive mining of difficult negatives)  \n- **Margin:** 0.3 (encourages wider separation between classes)\n\n## Visualization Option\n\n- Visualize all misclassified images after training is enabled.\n\n## Data Module Initialization\n\n- Loads training and validation datasets with specified directories and batch settings.  \n- Sets up data preprocessing and augmentation pipelines.\n\n## Model Initialization\n\n- Defines a binary classification model for gender classification.  \n- Applies hyperparameters including learning rate, weight decay, contrastive loss parameters, class weights, max epochs, label smoothing, and focal loss gamma.\n\n## Callbacks Configuration\n\n- Model Checkpoint: Saves the best-performing model based on validation accuracy.  \n- Early Stopping: Stops training if validation accuracy does not improve for 15 consecutive epochs.  \n- Learning Rate Monitor: Logs learning rate adjustments every epoch.\n\n## Logger Setup\n\n- Uses a CSV logger to store training logs under a designated folder with the experiment name.\n\n## Trainer Setup\n\n- Selects GPU for acceleration if available; otherwise CPU.  \n- Uses mixed precision training on GPU for efficiency.  \n- Logs training progress every 10 steps.  \n- Accumulates gradients over 2 batches to simulate a larger batch size.\n\n## Training Execution\n\n- Starts the training process using the configured trainer and data module.  \n- Prints messages indicating start and completion of training.\n\n## Post-training Evaluation\n\n- Loads the best model checkpoint after training completion.  \n- Generates detailed evaluation reports including classification metrics and confusion matrices on both training and validation sets.  \n- Visualizes misclassified images from the validation dataset, either all or a limited number based on the visualization flag.\n\n## Final Notes\n\n- Confirms when the full training, evaluation, and visualization pipeline has completed successfully.\n","metadata":{}},{"cell_type":"code","source":"\n# --- Main Training Script ---\nif __name__ == '__main__':\n    # Hyperparameters (you can tune these)\n    BATCH_SIZE = 16 # Keep this at 16 or lower due to two EfficientNet models\n    NUM_WORKERS = 4\n    LEARNING_RATE = 5e-5 # Adjusted for fine-tuning EfficientNet\n    WEIGHT_DECAY = 1e-5 # L2 regularization\n    MAX_EPOCHS = 50 # Increased epochs to allow Cosine Annealing to work\n    LABEL_SMOOTHING_EPSILON = 0.1 # Epsilon for label smoothing (common values are 0.05, 0.1, 0.2)\n    FOCAL_LOSS_GAMMA = 3.0 # Increased gamma for Focal Loss to focus more on hard examples\n\n    # Supervised Contrastive Loss parameters\n    SUPCON_TEMP = 0.07 \n    SUPCON_BASE_TEMP = 0.07 \n    SUPCON_HARD_MINING_RATIO = 0.5 # Increased for more aggressive hard mining\n    SUPCON_MARGIN = 0.3 # Increased for wider separation of negative pairs\n\n    # Flag to visualize all misclassified images\n    VISUALIZE_ALL_MISCLASSIFICATIONS = True \n\n    # Initialize DataModule\n    data_module = GenderDataModule(\n        train_data_dir=TRAIN_PATH,\n        val_data_dir=VAL_PATH,\n        train_preprocessed_data_dir=TRAIN_PREPROCESSED_PATH,\n        val_preprocessed_data_dir=VAL_PREPROCESSED_PATH,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS, # Consider increasing this if CPU is underutilized and you have more cores\n    )\n    data_module.setup('fit')\n\n    # Initialize Model\n    model = GenderClassificationModel(\n        num_classes=2,\n        learning_rate=LEARNING_RATE, # This learning rate will be used by the optimizer, then modulated by scheduler\n        weight_decay=WEIGHT_DECAY,\n        supcon_temp=SUPCON_TEMP,\n        supcon_base_temp=SUPCON_BASE_TEMP,\n        supcon_hard_mining_ratio=SUPCON_HARD_MINING_RATIO,\n        supcon_margin=SUPCON_MARGIN,\n        class_weights_for_loss=data_module.class_weights_for_loss,\n        class_weights_tensor=data_module.class_weights_tensor,\n        max_epochs=MAX_EPOCHS, # Pass MAX_EPOCHS to the model's constructor\n        label_smoothing=LABEL_SMOOTHING_EPSILON, # Pass label smoothing parameter\n        gamma=FOCAL_LOSS_GAMMA # Pass gamma to the model's constructor for Focal Loss\n    )\n\n    # Callbacks\n    # Checkpoint Callback: Save the best model based on validation accuracy\n    checkpoint_callback = ModelCheckpoint(\n        dirpath='checkpoints',\n        filename='best_model', # Default filename, can be customized with metrics\n        monitor='val_accuracy_epoch', # Monitoring val_accuracy_epoch now\n        mode='max',\n        save_top_k=1,\n        verbose=True\n    )\n    \n    # Early Stopping Callback: Stop if validation accuracy doesn't improve for 15 epochs\n    early_stopping_callback = EarlyStopping(\n        monitor='val_accuracy_epoch', # Monitor val_accuracy_epoch for early stopping\n        patience=15, \n        mode='max',\n        verbose=True\n    )\n    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n\n    # Logger\n    logger = CSVLogger(\"logs\", name=\"gender_classification\")\n\n    # Initialize Trainer\n    trainer = pl.Trainer(\n        max_epochs=MAX_EPOCHS,\n        logger=logger,\n        callbacks=[checkpoint_callback, early_stopping_callback, lr_monitor],\n        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n        devices=1, # Use 1 GPU if available\n        precision=16 if torch.cuda.is_available() else 32, # Mixed precision for faster training on GPU\n        log_every_n_steps=10, # Changed from 1 to 10 for slightly faster logging\n        accumulate_grad_batches=2 # Accumulate gradients over 2 batches to simulate larger batch size\n    )\n\n    print(\"\\nStarting model training...\")\n    trainer.fit(model, datamodule=data_module)\n    print(\"\\nTraining complete!\")\n\n    # --- Final Evaluation and Report Generation ---\n    if checkpoint_callback.best_model_path:\n        class_names = data_module.train_dataset.idx_to_class.values()\n        generate_detailed_reports(\n            checkpoint_callback.best_model_path, \n            data_module,\n            list(class_names)\n        )\n        # Determine how many misclassified images to visualize\n        num_images_to_viz = None if VISUALIZE_ALL_MISCLASSIFICATIONS else 10\n        \n        # Visualize misclassifications from the validation set using the best model\n        # Re-load the best model for visualization to ensure it's the one that generated the best performance\n        best_model_for_viz = GenderClassificationModel.load_from_checkpoint(\n            checkpoint_callback.best_model_path,\n            map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n            num_classes=2,\n            learning_rate=LEARNING_RATE,\n            weight_decay=WEIGHT_DECAY,\n            supcon_temp=SUPCON_TEMP,\n            supcon_base_temp=SUPCON_BASE_TEMP,\n            supcon_hard_mining_ratio=SUPCON_HARD_MINING_RATIO,\n            supcon_margin=SUPCON_MARGIN,\n            class_weights_for_loss=data_module.class_weights_for_loss,\n            class_weights_tensor=data_module.class_weights_tensor,\n            max_epochs=MAX_EPOCHS, # Pass MAX_EPOCHS to the model's constructor\n            label_smoothing=LABEL_SMOOTHING_EPSILON, # Pass label_smoothing\n            gamma=FOCAL_LOSS_GAMMA # Pass gamma when loading\n        )\n        val_dataloader_for_viz = data_module.val_dataloader()\n        visualize_misclassifications(\n            best_model_for_viz, \n            val_dataloader_for_viz, \n            list(class_names), \n            num_images=num_images_to_viz # Pass num_images to visualize\n        )\n    else:\n        print(\"No best model checkpoint found to generate final report or visualize misclassifications.\")\n\n    print(\"\\nPipeline execution complete!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}