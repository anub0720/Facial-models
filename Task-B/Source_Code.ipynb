{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-28T15:15:44.722906Z",
     "iopub.status.busy": "2025-06-28T15:15:44.722576Z",
     "iopub.status.idle": "2025-06-28T15:15:44.728483Z",
     "shell.execute_reply": "2025-06-28T15:15:44.727894Z",
     "shell.execute_reply.started": "2025-06-28T15:15:44.722880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "'''\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We begin by importing all the necessary libraries for building, training, and evaluating a deep learning model using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "# Using the high-performing EfficientNet-B4\n",
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import collections\n",
    "import math\n",
    "from sklearn.metrics import f1_score # Import F1 score utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è 1. Setup and Configuration\n",
    "\n",
    "We define a configuration dictionary that stores all the relevant hyperparameters and file paths used throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 1. Setup and Configuration ---\n",
    "\n",
    "# Configuration dictionary for hyperparameters and paths\n",
    "CONFIG = {\n",
    "    \"BASE_PATH\": \"/kaggle/input/comsys/Comys_Hackathon5/Task_B\", #\n",
    "    \"OUTPUT_PATH\": \"/kaggle/working/data\", #\n",
    "    \"BEST_MODEL_PATH\": \"/kaggle/working/data/best_embedding_model.pth\",\n",
    "    \"TRAIN_SPLIT_RATIO\": 0.8, #\n",
    "    \"BATCH_SIZE\": 16, # Kept smaller for the large EfficientNet model\n",
    "    \"EPOCHS\": 25, #\n",
    "    \"LEARNING_RATE\": 0.001, #\n",
    "    \"EMBEDDING_DIM\": 512,\n",
    "    # --- ArcFace Hyperparameters ---\n",
    "    \"ARCFACE_SCALE\": 30.0,\n",
    "    \"ARCFACE_MARGIN\": 0.5,\n",
    "}\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(CONFIG[\"OUTPUT_PATH\"], exist_ok=True) #\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Using device: {DEVICE} ---\") #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ 2. Data Preparation\n",
    "\n",
    "We define a function to split the original dataset into class-exclusive **train**, **validation**, and **test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. Data Preparation ---\n",
    "\n",
    "def prepare_datasets(base_path, output_path, train_split_ratio):\n",
    "    \"\"\"\n",
    "    Splits the original dataset into class-exclusive train, validation,\n",
    "    and test sets as per the requirements.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Data Preparation ---\")\n",
    "    \n",
    "    original_train_path = os.path.join(base_path, \"train\") #\n",
    "    original_val_path = os.path.join(base_path, \"val\") #\n",
    "    \n",
    "    final_train_path = os.path.join(output_path, \"train_final\") #\n",
    "    final_val_path = os.path.join(output_path, \"val_final\") #\n",
    "    final_test_path = os.path.join(output_path, \"test\") #\n",
    "\n",
    "    for path in [final_train_path, final_val_path, final_test_path]:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path) #\n",
    "        os.makedirs(path) #\n",
    "    \n",
    "    person_classes = [d for d in os.listdir(original_train_path) if os.path.isdir(os.path.join(original_train_path, d))] #\n",
    "    random.shuffle(person_classes) #\n",
    "    \n",
    "    split_index = int(len(person_classes) * train_split_ratio) #\n",
    "    train_classes = person_classes[:split_index] #\n",
    "    val_classes = person_classes[split_index:] #\n",
    "\n",
    "    print(f\"Splitting original train data: {len(train_classes)} classes for training, {len(val_classes)} for validation.\") #\n",
    "\n",
    "    for class_name in tqdm(train_classes, desc=\"Copying train classes\"):\n",
    "        shutil.copytree(os.path.join(original_train_path, class_name), os.path.join(final_train_path, class_name)) #\n",
    "        \n",
    "    for class_name in tqdm(val_classes, desc=\"Copying validation classes\"):\n",
    "        shutil.copytree(os.path.join(original_train_path, class_name), os.path.join(final_val_path, class_name)) #\n",
    "        \n",
    "    print(\"Preparing test set from original 'val' folder...\") #\n",
    "    test_classes = [d for d in os.listdir(original_val_path) if os.path.isdir(os.path.join(original_val_path, d))] #\n",
    "    for class_name in tqdm(test_classes, desc=\"Copying test classes\"):\n",
    "        shutil.copytree(os.path.join(original_val_path, class_name), os.path.join(final_test_path, class_name)) #\n",
    "        \n",
    "    print(\"--- Data Preparation Complete ---\") #\n",
    "    return final_train_path, final_val_path, final_test_path, len(train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† 3. Advanced Model Architecture with ArcFace\n",
    "\n",
    "## üîç EmbeddingNet\n",
    "\n",
    "The `EmbeddingNet` class uses the pretrained **EfficientNet-B4** model as a backbone for feature extraction. Instead of outputting class probabilities, the final classification layer is replaced with a fully connected layer that maps to a fixed-size embedding vector (e.g., 512 dimensions). This embedding is then used as input to a metric-learning-based classification layer such as ArcFace.\n",
    "\n",
    "The model also uses built-in input transforms from the EfficientNet weights, which ensure that the input images are normalized and resized appropriately.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© ArcMarginProduct: ArcFace Layer\n",
    "\n",
    "The `ArcMarginProduct` class implements the **ArcFace** loss function, which is designed for classification tasks that benefit from learning **discriminative features** ‚Äî such as face recognition or fine-grained visual identification.\n",
    "\n",
    "Instead of using raw logits for classification, ArcFace computes the cosine similarity between normalized embeddings and class weights. Then it adds an **angular margin penalty** to the similarity score of the correct class. This makes the decision boundary more strict and forces the network to produce embeddings that are:\n",
    "\n",
    "- **Close together** for samples from the same class\n",
    "- **Far apart** for samples from different classes\n",
    "\n",
    "The logits are scaled by a constant factor before applying softmax, which helps stabilize training.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "This architecture combines a strong image feature extractor (EfficientNet-B4) with a metric learning head (ArcFace), enabling more robust and meaningful embeddings for classification tasks. This is particularly effective in scenarios with a large number of classes and subtle differences between them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 3. Advanced Model Architecture with ArcFace ---\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"The core feature extractor network, using EfficientNet-B4.\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        weights = EfficientNet_B4_Weights.DEFAULT\n",
    "        self.backbone = efficientnet_b4(weights=weights)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Linear(in_features, embedding_dim)\n",
    "        self.transforms = weights.transforms()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.backbone(x)\n",
    "        return embedding\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    \"\"\"Implementation of ArcFace layer.\"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features, self.out_features, self.s, self.m = in_features, out_features, s, m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.cos_m, self.sin_m = math.cos(m), math.sin(m)\n",
    "        self.th, self.mm = math.cos(math.pi - m), math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, embedding, label):\n",
    "        cosine = F.linear(F.normalize(embedding), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = torch.zeros(cosine.size(), device=DEVICE)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è 4. Dataset for Classification\n",
    "\n",
    "## Custom Dataset: `FaceClassificationDataset`\n",
    "\n",
    "This class implements a PyTorch `Dataset` designed specifically for ArcFace training.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Data Organization:**\n",
    "  - Assumes the dataset is organized by class folders inside a root directory (`data_dir`).\n",
    "  - Each folder corresponds to a person/class and contains the images belonging to that class.\n",
    "  - Additionally, if a subfolder named `distortion` exists inside a class folder, images inside it are also included under the same class label.\n",
    "\n",
    "- **Class Label Mapping:**\n",
    "  - Classes (person names) are sorted alphabetically and mapped to integer indices.\n",
    "  - This mapping is stored in `class_to_idx` for consistent label encoding.\n",
    "\n",
    "- **Data Access:**\n",
    "  - `__len__` returns the total number of images.\n",
    "  - `__getitem__` loads an image by its index, applies optional transformations, and returns the `(image, class_label)` pair.\n",
    "\n",
    "### Usage:\n",
    "\n",
    "This dataset can be used with PyTorch DataLoaders to efficiently load and batch images during training. It supports data augmentation or normalization via the `transform` parameter.\n",
    "\n",
    "---\n",
    "\n",
    "This design ensures that images ‚Äî including those in distortion folders ‚Äî are correctly grouped by class for effective training with ArcFace loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 4. Dataset for Classification ---\n",
    "\n",
    "class FaceClassificationDataset(Dataset):\n",
    "    \"\"\"Custom dataset for ArcFace training. Returns (image, class_label).\"\"\"\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths, self.class_to_idx = [], {}\n",
    "        person_classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "        for i, class_name in enumerate(person_classes): self.class_to_idx[class_name] = i\n",
    "        for class_name, idx in self.class_to_idx.items():\n",
    "            class_path = os.path.join(self.data_dir, class_name)\n",
    "            distortion_path = os.path.join(class_path, 'distortion')\n",
    "            for img_name in os.listdir(class_path):\n",
    "                if img_name != 'distortion': self.image_paths.append((os.path.join(class_path, img_name), idx))\n",
    "            if os.path.exists(distortion_path):\n",
    "                for img_name in os.listdir(distortion_path): self.image_paths.append((os.path.join(distortion_path, img_name), idx))\n",
    "    def __len__(self): return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ 5. Training and Evaluation\n",
    "\n",
    "## Data Preparation for Evaluation\n",
    "\n",
    "- **prepare_evaluation_sets** scans the dataset directory once to gather:\n",
    "  - A **reference gallery**: clean images per class used as a stable set of embeddings for comparison.\n",
    "  - A **query set**: images (e.g., distorted or augmented) that need to be identified by the model.\n",
    "  - This avoids repeatedly scanning disk during evaluation, improving efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "- The evaluation function uses the pre-collected reference gallery and query images.\n",
    "- For each class in the reference gallery, embeddings are computed and averaged to form a class prototype vector.\n",
    "- Each query image embedding is compared against all class prototypes using cosine distance.\n",
    "- The closest prototype determines the predicted class.\n",
    "- Metrics computed:\n",
    "  - **Top-1 Accuracy**: Percentage of correct predictions.\n",
    "  - **Macro F1-Score**: F1 score averaged across classes, robust to class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Training Loop\n",
    "\n",
    "- The training loop jointly trains the feature extractor and the ArcFace classification head.\n",
    "- For each batch:\n",
    "  - Images are fed through the feature extractor to obtain embeddings.\n",
    "  - The ArcFace head produces margin-penalized logits from embeddings and labels.\n",
    "  - The loss is computed, gradients are backpropagated, and parameters updated.\n",
    "- After each epoch:\n",
    "  - The model is evaluated on the validation set.\n",
    "  - Learning rate scheduler adjusts based on validation accuracy.\n",
    "  - The best performing model checkpoint (based on accuracy) is saved.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This setup trains the model to learn discriminative embeddings with ArcFace margin penalties, and periodically evaluates using a reference-query retrieval approach to monitor performance with meaningful metrics like accuracy and macro F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 5. Training and Evaluation ---\n",
    "\n",
    "def prepare_evaluation_sets(data_path):\n",
    "    \"\"\"\n",
    "    Scans the data directory once to get the paths for reference and query images.\n",
    "    This avoids re-scanning the disk on every epoch.\n",
    "    \"\"\"\n",
    "    reference_gallery_paths = {}\n",
    "    query_set = []\n",
    "    person_classes = sorted([d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))])\n",
    "    \n",
    "    for class_name in person_classes:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        distortion_path = os.path.join(class_path, 'distortion')\n",
    "        clean_images = [os.path.join(class_path, f) for f in os.listdir(class_path) if f != 'distortion' and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if clean_images:\n",
    "            reference_gallery_paths[class_name] = clean_images\n",
    "        if os.path.exists(distortion_path):\n",
    "            for img_name in os.listdir(distortion_path):\n",
    "                query_set.append((os.path.join(distortion_path, img_name), class_name))\n",
    "    \n",
    "    return reference_gallery_paths, query_set, person_classes\n",
    "\n",
    "def evaluate(model, ref_gallery_paths, query_set, person_classes, transform, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model using pre-calculated file paths.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Evaluating Model ---\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    class_to_idx = {name: i for i, name in enumerate(person_classes)}\n",
    "\n",
    "    if not query_set:\n",
    "        print(\"No query images found to evaluate.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    print(\"Creating reference embedding gallery...\")\n",
    "    avg_reference_embeddings = {}\n",
    "    with torch.no_grad():\n",
    "        for class_name, img_paths in tqdm(ref_gallery_paths.items(), desc=\"Processing reference images\"):\n",
    "            # The model is run here, so embeddings are always up-to-date\n",
    "            embeddings = [model(transform(Image.open(p).convert(\"RGB\")).unsqueeze(0).to(device)) for p in img_paths]\n",
    "            avg_reference_embeddings[class_name] = torch.mean(torch.cat(embeddings), dim=0)\n",
    "\n",
    "    ref_labels = list(avg_reference_embeddings.keys())\n",
    "    ref_embeds = torch.stack(list(avg_reference_embeddings.values()))\n",
    "\n",
    "    print(\"Matching query images against gallery...\")\n",
    "    with torch.no_grad():\n",
    "        for query_path, true_label in tqdm(query_set, desc=\"Processing query images\"):\n",
    "            img_tensor = transform(Image.open(query_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            query_embedding = model(img_tensor)\n",
    "            distances = torch.cdist(F.normalize(query_embedding), F.normalize(ref_embeds))\n",
    "            best_match_idx = torch.argmin(distances, dim=1).item()\n",
    "            predicted_label = ref_labels[best_match_idx]\n",
    "            y_true.append(class_to_idx[true_label])\n",
    "            y_pred.append(class_to_idx[predicted_label])\n",
    "            \n",
    "    accuracy = (np.sum(np.array(y_true) == np.array(y_pred)) / len(y_true)) * 100\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"Evaluation Complete:\")\n",
    "    print(f\"  - Top-1 Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"  - Macro Avg F1-Score: {macro_f1:.4f}\")\n",
    "    return accuracy, macro_f1\n",
    "\n",
    "\n",
    "def train_model(feature_extractor, arcface_head, optimizer, criterion, scheduler, train_loader, val_gallery_paths, val_query_set, val_person_classes, transform, device, epochs, best_model_path):\n",
    "    print(\"\\n--- Starting Model Training with ArcFace ---\")\n",
    "    feature_extractor.to(device)\n",
    "    arcface_head.to(device)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        feature_extractor.train()\n",
    "        arcface_head.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = feature_extractor(images)\n",
    "            logits = arcface_head(embeddings, labels)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # --- Validation after each epoch ---\n",
    "        # Pass the pre-computed paths to the evaluate function\n",
    "        val_acc, val_f1 = evaluate(feature_extractor, val_gallery_paths, val_query_set, val_person_classes, transform, device)\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            print(f\"*** New best model found! Acc: {val_acc:.2f}%. Saving to {best_model_path} ***\")\n",
    "            torch.save(feature_extractor.state_dict(), best_model_path)\n",
    "            \n",
    "    print(\"--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ñ∂Ô∏è 6. Main Execution Block\n",
    "\n",
    "This is the entry point of the training and evaluation pipeline. It orchestrates the entire process as follows:\n",
    "\n",
    "## Dataset Preparation\n",
    "\n",
    "- Calls `prepare_datasets` to split the original data into training, validation, and test sets with class-exclusive splits.\n",
    "- Receives the number of training classes to configure the ArcFace head.\n",
    "\n",
    "## Model and Optimizer Setup\n",
    "\n",
    "- Instantiates the `EmbeddingNet` feature extractor and the `ArcMarginProduct` ArcFace head with appropriate dimensions and hyperparameters from `CONFIG`.\n",
    "- Defines the loss function (`CrossEntropyLoss`) and optimizer (`Adam`).\n",
    "- Sets up a learning rate scheduler (`ReduceLROnPlateau`) that reduces learning rate when validation accuracy plateaus.\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "- Creates a PyTorch DataLoader for the training set, applying the EfficientNet-specific data transforms.\n",
    "\n",
    "## Validation Set Preprocessing\n",
    "\n",
    "- Prepares validation gallery and query sets once before training to avoid repeated disk I/O during validation.\n",
    "- This optimization accelerates validation and evaluation phases.\n",
    "\n",
    "## Training\n",
    "\n",
    "- Calls the `train_model` function to train over the specified number of epochs.\n",
    "- The best model (based on validation accuracy) is saved to disk.\n",
    "\n",
    "## Final Testing\n",
    "\n",
    "- Prepares test set gallery and query paths similarly.\n",
    "- Loads the best saved model weights.\n",
    "- Runs final evaluation on the test set, reporting accuracy and F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This main block connects data preparation, model setup, training, and final evaluation seamlessly, ensuring efficient execution and robust performance monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T15:57:09.177533Z",
     "iopub.status.busy": "2025-06-28T15:57:09.176504Z",
     "iopub.status.idle": "2025-06-28T17:37:36.700274Z",
     "shell.execute_reply": "2025-06-28T17:37:36.699537Z",
     "shell.execute_reply.started": "2025-06-28T15:57:09.177503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 6. Main Execution Block ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path, val_path, test_path, num_train_classes = prepare_datasets(\n",
    "        CONFIG[\"BASE_PATH\"], CONFIG[\"OUTPUT_PATH\"], CONFIG[\"TRAIN_SPLIT_RATIO\"]\n",
    "    )\n",
    "    \n",
    "    feature_extractor = EmbeddingNet(embedding_dim=CONFIG[\"EMBEDDING_DIM\"])\n",
    "    arcface_head = ArcMarginProduct(\n",
    "        in_features=CONFIG[\"EMBEDDING_DIM\"], out_features=num_train_classes,\n",
    "        s=CONFIG[\"ARCFACE_SCALE\"], m=CONFIG[\"ARCFACE_MARGIN\"]\n",
    "    )\n",
    "    \n",
    "    data_transform = feature_extractor.transforms\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        list(feature_extractor.parameters()) + list(arcface_head.parameters()),\n",
    "        lr=CONFIG[\"LEARNING_RATE\"]\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "    train_dataset = FaceClassificationDataset(data_dir=train_path, transform=data_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=True, num_workers=2)\n",
    "    \n",
    "    # --- OPTIMIZATION: Prepare validation set paths ONCE before training ---\n",
    "    print(\"\\n--- Pre-calculating validation set file paths to optimize validation loop ---\")\n",
    "    val_gallery_paths, val_query_set, val_person_classes = prepare_evaluation_sets(val_path)\n",
    "    print(f\"Found {len(val_gallery_paths)} reference classes and {len(val_query_set)} query images in the validation set.\")\n",
    "\n",
    "    train_model(\n",
    "        feature_extractor, arcface_head, optimizer, criterion, scheduler,\n",
    "        train_loader, val_gallery_paths, val_query_set, val_person_classes, \n",
    "        data_transform, DEVICE, CONFIG[\"EPOCHS\"], CONFIG[\"BEST_MODEL_PATH\"]\n",
    "    )\n",
    "    \n",
    "    # --- Final evaluation using the BEST saved model ---\n",
    "    print(\"\\n--- Pre-calculating test set file paths for final evaluation ---\")\n",
    "    test_gallery_paths, test_query_set, test_person_classes = prepare_evaluation_sets(test_path)\n",
    "    print(f\"Found {len(test_gallery_paths)} reference classes and {len(test_query_set)} query images in the test set.\")\n",
    "\n",
    "    print(\"\\n--- Loading best model for final evaluation on Test Set ---\")\n",
    "    feature_extractor.load_state_dict(torch.load(CONFIG[\"BEST_MODEL_PATH\"]))\n",
    "    evaluate(feature_extractor, test_gallery_paths, test_query_set, test_person_classes, data_transform, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7671325,
     "sourceId": 12180300,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
