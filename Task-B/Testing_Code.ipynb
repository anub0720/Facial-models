{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUST AFTER IMPORTING LIBRARIES, THERE IS CONFIG. CHANGE CHECKPOINT PATH THERE (IF NEEDED). IN THE NEXT CODE CELL, CHANGE TEST FOLDER PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm # Use tqdm for notebooks/Colab\n",
    "import collections # Although not directly used in the snippet, good to include if it was in the original context\n",
    "import math\n",
    "from sklearn.metrics import f1_score # Import F1 score utility\n",
    "\n",
    "# --- 1. Setup and Configuration (Essential for Model Definition and Paths) ---\n",
    "CONFIG = {\n",
    "    \"BASE_PATH\": \"/kaggle/input/comsys/Comys_Hackathon5/Task_B\", # This might not be strictly needed for evaluation\n",
    "    \"OUTPUT_PATH\": \"/kaggle/working/data\", # Not strictly needed for evaluation but good to have\n",
    "    \"BEST_MODEL_PATH\": \"/kaggle/input/ckpt-for-comsystaska/best_embedding_model_TaskB.pth\", # Your provided best model path\n",
    "    \"EMBEDDING_DIM\": 512, # Crucial: Must match the embedding_dim used during training\n",
    "}\n",
    "\n",
    "# Ensure the output directory exists (if you plan to save anything, otherwise remove)\n",
    "os.makedirs(CONFIG[\"OUTPUT_PATH\"], exist_ok=True) # Good practice even if just evaluating\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Using device: {DEVICE} ---\")\n",
    "\n",
    "# --- 2. Required Model Architecture (Copy these classes here) ---\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"The core feature extractor network, using EfficientNet-B4.\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        weights = EfficientNet_B4_Weights.DEFAULT\n",
    "        self.backbone = efficientnet_b4(weights=weights)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        # Ensure this matches how you replaced the classifier during training\n",
    "        self.backbone.classifier = nn.Linear(in_features, embedding_dim)\n",
    "        self.transforms = weights.transforms()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.backbone(x)\n",
    "        return embedding\n",
    "\n",
    "# NOTE: ArcMarginProduct is NOT needed for inference/evaluation, as it's part of the classification head.\n",
    "# The EmbeddingNet itself is what generates the embeddings for similarity comparison.\n",
    "# class ArcMarginProduct(nn.Module):\n",
    "#     # ... (definition from your original code) ...\n",
    "#     pass\n",
    "\n",
    "# --- 3. Required Data Preparation Functions (Copy these functions here) ---\n",
    "# This function is crucial for organizing your test data for evaluation.\n",
    "def prepare_evaluation_sets(data_path):\n",
    "    \"\"\"\n",
    "    Scans the data directory once to get the paths for reference and query images.\n",
    "    This avoids re-scanning the disk on every epoch.\n",
    "    Assumes a structure like:\n",
    "    data_path/\n",
    "    ├── ClassA/\n",
    "    │   ├── img1.jpg (reference)\n",
    "    │   └── distortion/\n",
    "    │       └── distorted_img1.jpg (query)\n",
    "    └── ClassB/\n",
    "        ├── img1.jpg (reference)\n",
    "        └── distortion/\n",
    "            └── distorted_img1.jpg (query)\n",
    "    \"\"\"\n",
    "    reference_gallery_paths = {}\n",
    "    query_set = []\n",
    "    \n",
    "    # Ensure data_path exists and is a directory\n",
    "    if not os.path.exists(data_path) or not os.path.isdir(data_path):\n",
    "        print(f\"Warning: Data path does not exist or is not a directory: {data_path}\")\n",
    "        return {}, [], []\n",
    "\n",
    "    person_classes = sorted([d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))])\n",
    "    \n",
    "    for class_name in person_classes:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        distortion_path = os.path.join(class_path, 'distortion')\n",
    "        \n",
    "        # Collect clean images for reference gallery\n",
    "        clean_images = [os.path.join(class_path, f) for f in os.listdir(class_path)\n",
    "                        if f != 'distortion' and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if clean_images:\n",
    "            reference_gallery_paths[class_name] = clean_images\n",
    "        \n",
    "        # Collect distorted images for query set\n",
    "        if os.path.exists(distortion_path) and os.path.isdir(distortion_path):\n",
    "            for img_name in os.listdir(distortion_path):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    query_set.append((os.path.join(distortion_path, img_name), class_name))\n",
    "    \n",
    "    return reference_gallery_paths, query_set, person_classes\n",
    "\n",
    "# --- 4. Required Evaluation Function (Copy this function here) ---\n",
    "def evaluate(model, ref_gallery_paths, query_set, person_classes, transform, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model using pre-calculated file paths.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Evaluating Model ---\")\n",
    "    model.to(device)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    class_to_idx = {name: i for i, name in enumerate(person_classes)}\n",
    "\n",
    "    if not query_set:\n",
    "        print(\"No query images found to evaluate. Check your data structure or prepare_evaluation_sets.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    print(\"Creating reference embedding gallery...\")\n",
    "    avg_reference_embeddings = collections.OrderedDict() # Use OrderedDict for consistent ordering\n",
    "    with torch.no_grad():\n",
    "        for class_name, img_paths in tqdm(ref_gallery_paths.items(), desc=\"Processing reference images\"):\n",
    "            embeddings = []\n",
    "            for p in img_paths:\n",
    "                try:\n",
    "                    img = Image.open(p).convert(\"RGB\")\n",
    "                    tensor_img = transform(img).unsqueeze(0).to(device)\n",
    "                    embeddings.append(model(tensor_img))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing reference image {p}: {e}\")\n",
    "                    continue # Skip this image and continue\n",
    "            if embeddings: # Only average if there were successful embeddings\n",
    "                avg_reference_embeddings[class_name] = torch.mean(torch.cat(embeddings), dim=0)\n",
    "            else:\n",
    "                print(f\"Warning: No valid reference images for class {class_name}. Skipping.\")\n",
    "\n",
    "    if not avg_reference_embeddings:\n",
    "        print(\"No valid reference embeddings created. Cannot evaluate.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    ref_labels = list(avg_reference_embeddings.keys())\n",
    "    ref_embeds = torch.stack(list(avg_reference_embeddings.values()))\n",
    "    \n",
    "    print(\"Matching query images against gallery...\")\n",
    "    with torch.no_grad():\n",
    "        for query_path, true_label in tqdm(query_set, desc=\"Processing query images\"):\n",
    "            try:\n",
    "                img_tensor = transform(Image.open(query_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "                query_embedding = model(img_tensor)\n",
    "                \n",
    "                # Calculate cosine similarity (more common for face recognition)\n",
    "                # Ensure embeddings are normalized if not already by the model\n",
    "                normalized_query = F.normalize(query_embedding)\n",
    "                normalized_refs = F.normalize(ref_embeds)\n",
    "                \n",
    "                # Cosine similarity: (Q . R) / (||Q|| * ||R||) -> simplified to Q . R if normalized\n",
    "                similarities = torch.matmul(normalized_query, normalized_refs.T)\n",
    "                \n",
    "                # For similarity, higher is better, so argmax. For distance (cdist), argmin.\n",
    "                best_match_idx = torch.argmax(similarities, dim=1).item()\n",
    "                \n",
    "                predicted_label = ref_labels[best_match_idx]\n",
    "                y_true.append(class_to_idx[true_label])\n",
    "                y_pred.append(class_to_idx[predicted_label])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query image {query_path}: {e}\")\n",
    "                # Decide how to handle this: skip, or add a 'failure' prediction\n",
    "                # For now, we skip, which means y_true and y_pred might be shorter than query_set initially\n",
    "                continue\n",
    "\n",
    "    if not y_true: # If no successful queries were processed\n",
    "        print(\"No successful query predictions made. Cannot calculate metrics.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    accuracy = (np.sum(np.array(y_true) == np.array(y_pred)) / len(y_true)) * 100\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"Evaluation Complete:\")\n",
    "    print(f\"  - Top-1 Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"  - Macro Avg F1-Score: {macro_f1:.4f}\")\n",
    "    return accuracy, macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTER TEST DATASET PATH IN THE NEXT CELL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Main Execution Block for Standalone Evaluation ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Starting Standalone Evaluation on a NEW, Separate Test Folder ---\")\n",
    "\n",
    "    # Define the path to your new test folder\n",
    "    # !!! IMPORTANT: REPLACE THIS WITH THE ACTUAL PATH TO YOUR NEW TEST FOLDER !!!\n",
    "    NEW_TEST_FOLDER_PATH = \"/kaggle/input/comsys/Comys_Hackathon5/Task_B/val\" # Example: /path/to/your/new_test_dataset\n",
    "\n",
    "    if not os.path.exists(NEW_TEST_FOLDER_PATH):\n",
    "        print(f\"Error: The specified NEW_TEST_FOLDER_PATH does not exist: {NEW_TEST_FOLDER_PATH}\")\n",
    "        print(\"Please update NEW_TEST_FOLDER_PATH to your actual test data location and ensure it's accessible.\")\n",
    "        exit() # Exit if the test path is invalid\n",
    "\n",
    "    # Prepare evaluation sets for the new test folder\n",
    "    print(f\"\\n--- Pre-calculating file paths for the NEW Test Set: {NEW_TEST_FOLDER_PATH} ---\")\n",
    "    new_test_gallery_paths, new_test_query_set, new_test_person_classes = prepare_evaluation_sets(NEW_TEST_FOLDER_PATH)\n",
    "    print(f\"Found {len(new_test_gallery_paths)} reference classes and {len(new_test_query_set)} query images in the NEW test set.\")\n",
    "\n",
    "    if not new_test_query_set:\n",
    "        print(\"No query images found in the NEW test folder to evaluate. Ensure 'distortion' subfolders exist if intended for queries.\")\n",
    "    elif not new_test_gallery_paths:\n",
    "        print(\"No reference images found in the NEW test folder to create a gallery. Evaluation cannot proceed.\")\n",
    "    else:\n",
    "        # Load the BEST saved model for evaluation on the new test set\n",
    "        print(\"\\n--- Loading best model for evaluation ---\")\n",
    "        \n",
    "        # Instantiate a fresh model to load weights into.\n",
    "        eval_feature_extractor = EmbeddingNet(embedding_dim=CONFIG[\"EMBEDDING_DIM\"])\n",
    "        \n",
    "        # Load the state dictionary from the provided path\n",
    "        try:\n",
    "            # map_location ensures it loads correctly whether on CPU or GPU\n",
    "            eval_feature_extractor.load_state_dict(torch.load(CONFIG[\"BEST_MODEL_PATH\"], map_location=DEVICE))\n",
    "            print(f\"Successfully loaded model from {CONFIG['BEST_MODEL_PATH']}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Best model file not found at {CONFIG['BEST_MODEL_PATH']}. Please ensure the path is correct and the file exists.\")\n",
    "            exit()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading the model: {e}\")\n",
    "            exit()\n",
    "\n",
    "        eval_feature_extractor.to(DEVICE) # Move the model to the chosen device\n",
    "\n",
    "        # Get the transformation used by the EfficientNet model\n",
    "        # This is crucial: EfficientNet expects specific input preprocessing.\n",
    "        data_transform = EfficientNet_B4_Weights.DEFAULT.transforms()\n",
    "\n",
    "        # Perform the evaluation using the loaded model\n",
    "        print(f\"\\n--- Evaluating on NEW Test Folder: {NEW_TEST_FOLDER_PATH} ---\")\n",
    "        evaluate(eval_feature_extractor, new_test_gallery_paths, new_test_query_set, new_test_person_classes, data_transform, DEVICE)\n",
    "\n",
    "print(\"\\n--- Standalone Evaluation Process Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7671325,
     "sourceId": 12180300,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7794333,
     "sourceId": 12363077,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
